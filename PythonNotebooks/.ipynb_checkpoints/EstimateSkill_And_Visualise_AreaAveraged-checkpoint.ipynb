{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/users/lfbor/data\n"
     ]
    }
   ],
   "source": [
    "# ===============\n",
    "# To 1) choose a method of combining/bias correcting the chosen analogues, and\n",
    "#    2) calculate/plot the skill against other baselines\n",
    "# Note: For the original paper, the ANNUAL version was used (not the trend version)\n",
    "# ===============\n",
    "import getpass\n",
    "usr = getpass.getuser()\n",
    "\n",
    "figure_dir = '/home/users/{}/python/output_figures/'.format(usr)\n",
    "data_dir = '/home/users/{}/data'.format(usr)\n",
    "processed_output_dir = '/home/users/{}/data/'.format(usr)\n",
    "scripts_dir = '/home/users/{}/python/scripts3/'.format(usr)\n",
    "\n",
    "print(data_dir)\n",
    "\n",
    "# TODO: NOTE (Nov 2020)\n",
    "# I don't think I am consistent with \"chosen_num_mem\" here. i.e. Sometimes I count from the end, backwards. Other\n",
    "# times I seem to count forwards. If chosen_num_mem is 20 or greater then I am using all of the members in the file\n",
    "# so it's okay, but I should be more careful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============\n",
    "# Various choices\n",
    "# ===============\n",
    "analogue_var = 'SST'\n",
    "forecast_var = 'SST'\n",
    "\n",
    "picontrols_only = False\n",
    "skip_local_hist = False\n",
    "strong_forcing_only = False\n",
    "\n",
    "# just_plot_annual = False\n",
    "do_slow_plots = False\n",
    "\n",
    "stop_analogues_early_like_dcpp = False  # This is pointless as the last actual forecast of LT2-10 is from 2009 anyway\n",
    "\n",
    "# ===============\n",
    "# These correspond to the output (file name) from STEP3a\n",
    "# ===============\n",
    "# target_regions  = ['north_atlantic', 'subpolar_gyre']\n",
    "# target_domain_strings = ['+65+00+00-90', '+65+10+45-60', '+50+00+40-70', '+30+20+00-85', '+30+20+05-60']\n",
    "# windows = [1, 2, 3, 5, 7, 10, 15, 25, 35, 45]\n",
    "# smoothings = [1, 11, 21]\n",
    "chosen_target_region = 'subpolar_gyre'  # The region we are computing the skill for\n",
    "chosen_target_domain = '+65+00+00-90'   # The region we used when choosing the \"best\" analogues\n",
    "chosen_window = 35                      # The window over which we chose the best analogues (trends over this period)\n",
    "chosen_smoothing = 5                    # The length of any prior smoothing applied\n",
    "chosen_num_mem = 100                    # The number of analogue memebrs to allow into the ensemble each time\n",
    "chosen_norm_window = 35                 # IMPORTANT! This is the window over which we (might) scale the variance and is not necessarily the same as the analogue creation window defined above\n",
    "clim_start, clim_end = 1960, 1990       # The climatology period we used (e.g. all models/experiments (and obs) had this (historical) period time-mean removed)\n",
    "rmse_method = True                      # Whether our skill metric was RMSE or correlation\n",
    "\n",
    "# ===============\n",
    "# Which of the predefined methods of bias correction (or \"re-centering\" to use)\n",
    "# ===============\n",
    "old_recentre_method = False\n",
    "new_recentre_method = False\n",
    "new_recentre_method2 = False\n",
    "new_recentre_method2b = False\n",
    "simple_recentre_method = False\n",
    "simpleScaled_recentre_method = False\n",
    "clever_scaling_method = False  # Time series look nice\n",
    "clever_scaling_methodb = False\n",
    "clever_scaling_methodc = True  #  (New TS Map method does the same as this (assuming divide_by_sd=FALSE in maps))\n",
    "# clever_scaling_methodd = True  #  As C but shifting by mean over previous chosen_window rather than t=0\n",
    "# clever_scaling_methode = False  #  As D but also normalising MMM over that window rather than full analogue window\n",
    "map_method = False  # (Old map method)\n",
    "map_method_nosd = False\n",
    "\n",
    "do_ltbc = True\n",
    "\n",
    "hadisst_spg_2020 = 9.441540503229705 # Estimated from ReadHadISST_ASCII.ipynb. TODO: Update hadisst data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'Panel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ddb7038c610c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#import xarray as xr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxarray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopen_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/jaspy/lib/python3.7/site-packages/xarray/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malignment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfull_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mones_like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_combine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapply_ufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m from .core.extensions import (register_dataarray_accessor,\n",
      "\u001b[0;32m/opt/jaspy/lib/python3.7/site-packages/xarray/core/combine.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0malignment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpycompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteritems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexVariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/jaspy/lib/python3.7/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mas_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massert_unique_multiindex_level_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mPANDAS_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPanel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m _VALID_COMPAT = Frozen({'identical': 0,\n",
      "\u001b[0;32m/opt/jaspy/lib/python3.7/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_SparseArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module 'pandas' has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'Panel'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "sys.path.insert(0, '/home/users/lfbor/python/scripts3/python_modules')\n",
    "from analogue import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "if (clim_start != 1960) or (clim_end != 1990):\n",
    "    clim_string = '_Clim{:d}{:d}'.format(clim_start, clim_end)\n",
    "    processed_output_dir += '_{:d}-{:d}'.format(clim_start, clim_end)\n",
    "else:\n",
    "    clim_string = ''\n",
    "    \n",
    "hadisst_save_file = '{:s}/HadISST_time_series_regions.pkl'.format(data_dir)\n",
    "baseline_save_file = '{:s}/{:s}_BaselineSkillMeasures_{:s}.pkl'.format(data_dir, forecast_var, chosen_target_region)\n",
    "\n",
    "print(baseline_save_file)\n",
    "    \n",
    "en4_save_file = '{:s}/EN4_0-500m_time_series_regions.pkl'.format(data_dir)\n",
    "hadcrut4_save_file = '{:s}/HadCRUT4_time_series_regions.pkl'.format(data_dir)\n",
    "\n",
    "max_mems_to_take = 20\n",
    "if chosen_num_mem > max_mems_to_take:\n",
    "    max_mems_to_take = chosen_num_mem\n",
    "nlead = 11\n",
    "lead_times = np.arange(nlead)\n",
    "norm_windows = np.array([1, 2, 3, 4, 5, 6, 8, 10, 12, 15, 18, 20, 25, 30, 35, 45])\n",
    "ichosen_norm_window = np.argwhere(norm_windows == chosen_norm_window)\n",
    "if len(ichosen_norm_window) == 0: raise ValueError('Must choose from the allowed norm_windows')\n",
    "ichosen_norm_window = ichosen_norm_window[0][0]\n",
    "# def read_target_domain(in_string):\n",
    "#     out_list = []\n",
    "#     for ii in range(4):\n",
    "#         out_list.append(np.int(in_string[ii*3:(ii+1)*3]))\n",
    "#     return out_list\n",
    "\n",
    "# Read the historical data for validation\n",
    "if forecast_var == 'SST':\n",
    "    forecast_save_file = hadisst_save_file\n",
    "elif forecast_var == 'DepthAverageT':\n",
    "    forecast_save_file = en4_save_file\n",
    "elif forecast_var == 'SAT':\n",
    "    forecast_save_file = hadcrut4_save_file\n",
    "else:\n",
    "    raise ValueError('Unknown variable')\n",
    "with open(forecast_save_file, 'rb') as handle:\n",
    "    print(\"Loading save file: {:s}\".format(forecast_save_file))\n",
    "    _, forecast_obs, _, year_forecast_obs = pickle.load(handle, encoding='latin')\n",
    "    forecast_obs = forecast_obs[chosen_target_region]\n",
    "    nyrs = len(year_forecast_obs)\n",
    "print('nyrs', nyrs)\n",
    "\n",
    "if old_recentre_method:\n",
    "    method_string = '_OLD'\n",
    "elif new_recentre_method:\n",
    "    method_string = '_NEW'\n",
    "elif new_recentre_method2:\n",
    "    method_string = '_NEW2'\n",
    "elif simple_recentre_method:\n",
    "    method_string = '_SIMPLE'\n",
    "elif simpleScaled_recentre_method:\n",
    "    method_string = '_SIMPLESCALED'\n",
    "elif clever_scaling_method:\n",
    "    method_string = '_CLEVER'\n",
    "elif clever_scaling_methodb:\n",
    "    method_string = '_CLEVERB'\n",
    "elif clever_scaling_methodc:\n",
    "    method_string = '_CLEVERC'\n",
    "elif map_method:\n",
    "    method_string = '_MAP'\n",
    "elif map_method_nosd:\n",
    "    method_string = '_MAPNOSD'\n",
    "else:\n",
    "    method_string = ''\n",
    "    \n",
    "if rmse_method:\n",
    "    rmse_string = '_RMSEmethod'\n",
    "else:\n",
    "    rmse_string = ''\n",
    "    \n",
    "if chosen_smoothing > 1:\n",
    "    smo_len = chosen_smoothing\n",
    "    smoothing_string = '_Smo{:d}'.format(smo_len)\n",
    "else:\n",
    "    smoothing_string = ''\n",
    "    \n",
    "if do_ltbc:\n",
    "    ltbc_string = '_LTBC'\n",
    "else:\n",
    "    ltbc_string = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(baseline_save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmip5_list_file = os.path.join(scripts_dir, 'model_lists/cmip5_list.txt')\n",
    "cmip6_list_file = os.path.join(scripts_dir, 'model_lists/cmip6_list.txt')\n",
    "\n",
    "cmip5_models = []\n",
    "with open(cmip5_list_file, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        cmip5_models.append(line.strip())\n",
    "\n",
    "cmip6_models = []\n",
    "with open(cmip6_list_file, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        cmip6_models.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_lead = [1, 2, 3, 4, 5, 6,  1,  2]  # For the multiannual skill\n",
    "end_lead =   [5, 6, 7, 8, 9, 10, 10, 10]\n",
    "nlead_multi = len(start_lead)\n",
    "lead_times_multi = np.arange(nlead_multi)\n",
    "running_skill_window = 35\n",
    "\n",
    "xlim = (year_forecast_obs[0], year_forecast_obs[-1]+nlead)\n",
    "labels_multi = ['-'.join([str(ss), str(ee)]) for ss, ee in zip(start_lead, end_lead)]\n",
    "\n",
    "gs1 = gridspec.GridSpec(5, 1)\n",
    "gs1.update(wspace=0.2)\n",
    "\n",
    "# ===============\n",
    "# Read the processed analogue data. This is the output of STEP3a\n",
    "# ===============\n",
    "analogue_base_info = 'ANALOGUE{:s}_FORECAST{:s}_DOMAIN{:s}_TARGET{:s}_WINDOW{:d}_MEMS{:d}{:s}_SpatialSkill{:s}{:s}{:s}{:s}_info'\n",
    "analogue_base_info = analogue_base_info.format(analogue_var, forecast_var, chosen_target_domain, chosen_target_region, chosen_window, max_mems_to_take, smoothing_string, '', '', rmse_string, '')\n",
    "analogue_base_forecast = 'ANALOGUE{:s}_FORECAST{:s}_DOMAIN{:s}_TARGET{:s}_WINDOW{:d}_MEMS{:d}{:s}_SpatialSkill{:s}{:s}{:s}{:s}_forecast'\n",
    "analogue_base_forecast = analogue_base_forecast.format(analogue_var, forecast_var, chosen_target_domain, chosen_target_region, chosen_window, max_mems_to_take, smoothing_string, '', '', rmse_string, '')\n",
    "analogue_base_means = 'ANALOGUE{:s}_FORECAST{:s}_DOMAIN{:s}_TARGET{:s}_WINDOW{:d}_MEMS{:d}{:s}_SpatialSkill{:s}{:s}{:s}{:s}_means'\n",
    "analogue_base_means = analogue_base_means.format(analogue_var, forecast_var, chosen_target_domain, chosen_target_region, chosen_window, max_mems_to_take, smoothing_string, '', '', rmse_string, '')\n",
    "print(analogue_base_forecast)\n",
    "\n",
    "picontrols_string = ''\n",
    "hist_string = ''\n",
    "strong_string = ''\n",
    "if picontrols_only:\n",
    "    picontrols_string = '_piControlsOnly'\n",
    "elif skip_local_hist:\n",
    "    nearby_hist = 75\n",
    "    hist_string += '_SkipLocalHist{:d}'.format(nearby_hist)\n",
    "elif strong_forcing_only:\n",
    "    earliest_hist = 1990\n",
    "    strong_string += '_StrongForcing{:d}'.format(earliest_hist)\n",
    "\n",
    "analogue_base_info += (picontrols_string + hist_string + strong_string)\n",
    "analogue_base_info += '.nc'\n",
    "analogue_file_info = os.path.join(processed_output_dir, analogue_base_info)\n",
    "analogue_base_forecast += (picontrols_string + hist_string + strong_string)\n",
    "analogue_base_forecast += '.nc'\n",
    "analogue_file_forecast = os.path.join(processed_output_dir, analogue_base_forecast)\n",
    "analogue_base_means += (picontrols_string + hist_string + strong_string)\n",
    "analogue_base_means += '.nc'\n",
    "analogue_file_means = os.path.join(processed_output_dir, analogue_base_means)\n",
    "\n",
    "if not os.path.isfile(analogue_file_info):\n",
    "    raise ValueError(\"No save file: {:s}\".format(analogue_file_info))\n",
    "\n",
    "print(\"Reading: {:s}\".format(analogue_file_info))\n",
    "ds_info = xr.open_dataset(analogue_file_info).to_array()\n",
    "ds_data_info = np.ma.masked_array(ds_info.values)\n",
    "ann_corr_info = ds_data_info[0]\n",
    "trend_corr_info = ds_data_info[1]\n",
    "print(ann_corr_info)\n",
    "\n",
    "print(\"Reading: {:s}\".format(analogue_file_forecast))\n",
    "ds_forecast = xr.open_dataset(analogue_file_forecast).to_array()\n",
    "ds_data_forecast = np.ma.masked_array(ds_forecast.values)\n",
    "ann_forecast = ds_data_forecast[0]\n",
    "trend_forecast = ds_data_forecast[1]\n",
    "print(ann_forecast)\n",
    "\n",
    "print(\"Reading: {:s}\".format(analogue_file_means))\n",
    "ds_means = xr.open_dataset(analogue_file_means).to_array()\n",
    "ds_data_means = np.ma.masked_array(ds_means.values)\n",
    "ann_forecast_means = ds_data_means[0]\n",
    "ann_forecast_sds = ds_data_means[1]\n",
    "trend_forecast_means = ds_data_means[2]\n",
    "trend_forecast_sds = ds_data_means[3]\n",
    "print(ann_forecast_means)\n",
    "\n",
    "print(ann_corr_info.shape, ann_forecast.shape, ann_forecast_means.shape, ann_forecast_sds.shape)\n",
    "print(\"Fixing ann_corr_info mask !! This should have been fixed but apparently not. Some rogue/bad SpatialProcessed files probable !!\")\n",
    "#ann_corr_info[:chosen_window-1, :, :].mask = True\n",
    "ann_corr_info[:chosen_window-1, :].mask = True\n",
    "ann_forecast[:chosen_window-1, :, :].mask = True\n",
    "ann_forecast_means[:chosen_window-1, :].mask = True\n",
    "ann_forecast_sds[:chosen_window-1, :].mask = True\n",
    "    \n",
    "\n",
    "#     Keep just the means over the chosen normalisation window (not necessarily same as analogue creation window)\n",
    "print(ann_forecast_means.shape)\n",
    "ann_forecast_means = ann_forecast_means[:, :, ichosen_norm_window]\n",
    "ann_forecast_sds = ann_forecast_sds[:, :, ichosen_norm_window]\n",
    "trend_forecast_means = trend_forecast_means[:, :, ichosen_norm_window]\n",
    "trend_forecast_sds = trend_forecast_sds[:, :, ichosen_norm_window]\n",
    "\n",
    "if stop_analogues_early_like_dcpp:\n",
    "    stop_string = '_TruncatedAnalogueLikeDCPP'\n",
    "    # To make the same as the hindcasts\n",
    "    ann_forecast[146, :, :] = ann_forecast[0, :, :]\n",
    "    ann_forecast[147, :, :] = ann_forecast[0, :, :]\n",
    "    ann_forecast[148, :, :] = ann_forecast[0, :, :]\n",
    "else:\n",
    "    stop_string = ''\n",
    "\n",
    "# ===============\n",
    "# Make a list of the input data files, in case I want to read the raw data in and investigate it\n",
    "# ===============\n",
    "output_filelist_base = 'InputFilesList_{:s}_DOMAIN{:s}_TARGET{:s}_WINDOW{:d}_MEMS{:d}{:s}_SpatialSkill{:s}{:s}{:s}.nc'\n",
    "analogue_output_filelist = output_filelist_base.format(analogue_var, chosen_target_domain, chosen_target_region,\n",
    "                                                       chosen_window, chosen_num_mem, smoothing_string,\n",
    "                                                       '', '', rmse_string)\n",
    "forecast_output_filelist = output_filelist_base.format(forecast_var, chosen_target_domain, chosen_target_region,\n",
    "                                                       chosen_window, chosen_num_mem, smoothing_string,\n",
    "                                                       '', '', rmse_string)\n",
    "analogue_output_filelist = os.path.join(data_dir, analogue_output_filelist)\n",
    "forecast_output_filelist = os.path.join(data_dir, forecast_output_filelist)\n",
    "\n",
    "# target_domain = read_target_domain(chosen_target_domain)\n",
    "fig_save_file1 = '{:s}HistoricalAnalogues_Spatial_Trend_Input_ANALOGUE{:s}_FORECAST{:s}_DOMAIN{:s}_TARGET{:s}_WINDOW{:d}_MEMS{:d}_NORM{:d}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}.png'\n",
    "fig_save_file1 = fig_save_file1.format(figure_dir, analogue_var, forecast_var, chosen_target_domain, chosen_target_region, chosen_window, chosen_num_mem, chosen_norm_window, smoothing_string, '', '', rmse_string, method_string, picontrols_string, hist_string, strong_string, ltbc_string, clim_string, stop_string)\n",
    "fig_save_file2 = '{:s}HistoricalAnalogues_Spatial_Trend_LTDS_ANALOGUE{:s}_FORECAST{:s}_DOMAIN{:s}_TARGET{:s}_WINDOW{:d}_MEMS{:d}_NORM{:d}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}.png'\n",
    "fig_save_file2 = fig_save_file2.format(figure_dir, analogue_var, forecast_var, chosen_target_domain, chosen_target_region, chosen_window, chosen_num_mem, chosen_norm_window, smoothing_string, '', '', rmse_string, method_string, picontrols_string, hist_string, strong_string, ltbc_string, clim_string, stop_string)\n",
    "fig_save_file3 = '{:s}HistoricalAnalogues_Spatial_Trend_Expts_ANALOGUE{:s}_FORECAST{:s}_DOMAIN{:s}_TARGET{:s}_WINDOW{:d}_MEMS{:d}_NORM{:d}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}.png'\n",
    "fig_save_file3 = fig_save_file3.format(figure_dir, analogue_var, forecast_var, chosen_target_domain, chosen_target_region, chosen_window, chosen_num_mem, chosen_norm_window, smoothing_string, '', '', rmse_string, method_string, picontrols_string, hist_string, strong_string, ltbc_string, clim_string, stop_string)\n",
    "\n",
    "fig_save_file1b = '{:s}HistoricalAnalogues_Spatial_Ann_Input_ANALOGUE{:s}_FORECAST{:s}_DOMAIN{:s}_TARGET{:s}_WINDOW{:d}_MEMS{:d}_NORM{:d}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}.png'\n",
    "fig_save_file1b = fig_save_file1b.format(figure_dir, analogue_var, forecast_var, chosen_target_domain, chosen_target_region, chosen_window, chosen_num_mem, chosen_norm_window, smoothing_string, '', '', rmse_string, method_string, picontrols_string, hist_string, strong_string, ltbc_string, clim_string, stop_string)\n",
    "fig_save_file1c = '{:s}HistoricalAnalogues_Spatial_Ann_InputExpts_ANALOGUE{:s}_FORECAST{:s}_DOMAIN{:s}_TARGET{:s}_WINDOW{:d}_MEMS{:d}_NORM{:d}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}.png'\n",
    "fig_save_file1c = fig_save_file1c.format(figure_dir, analogue_var, forecast_var, chosen_target_domain, chosen_target_region, chosen_window, chosen_num_mem, chosen_norm_window, smoothing_string, '', '', rmse_string, method_string, picontrols_string, hist_string, strong_string, ltbc_string, clim_string, stop_string)\n",
    "fig_save_file1d = '{:s}HistoricalAnalogues_Spatial_Ann_InputExpts2_ANALOGUE{:s}_FORECAST{:s}_DOMAIN{:s}_TARGET{:s}_WINDOW{:d}_MEMS{:d}_NORM{:d}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}.png'\n",
    "fig_save_file1d = fig_save_file1d.format(figure_dir, analogue_var, forecast_var, chosen_target_domain, chosen_target_region, chosen_window, chosen_num_mem, chosen_norm_window, smoothing_string, '', '', rmse_string, method_string, picontrols_string, hist_string, strong_string, ltbc_string, clim_string, stop_string)\n",
    "fig_save_file2b = '{:s}HistoricalAnalogues_Spatial_Ann_LTDS_ANALOGUE{:s}_FORECAST{:s}_DOMAIN{:s}_TARGET{:s}_WINDOW{:d}_MEMS{:d}_NORM{:d}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}.png'\n",
    "fig_save_file2b = fig_save_file2b.format(figure_dir, analogue_var, forecast_var, chosen_target_domain, chosen_target_region, chosen_window, chosen_num_mem, chosen_norm_window, smoothing_string, '', '', rmse_string, method_string, picontrols_string, hist_string, strong_string, ltbc_string, clim_string, stop_string)\n",
    "fig_save_file3b = '{:s}HistoricalAnalogues_Spatial_Ann_Expts_ANALOGUE{:s}_FORECAST{:s}_DOMAIN{:s}_TARGET{:s}_WINDOW{:d}_MEMS{:d}_NORM{:d}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}.png'\n",
    "fig_save_file3b = fig_save_file3b.format(figure_dir, analogue_var, forecast_var, chosen_target_domain, chosen_target_region, chosen_window, chosen_num_mem, chosen_norm_window, smoothing_string, '', '', rmse_string, method_string, picontrols_string, hist_string, strong_string, ltbc_string, clim_string, stop_string)\n",
    "fig_save_file4 = '{:s}HistoricalAnalogues_Spatial_Ann_Density_ANALOGUE{:s}_FORECAST{:s}_DOMAIN{:s}_TARGET{:s}_WINDOW{:d}_MEMS{:d}_NORM{:d}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}{:s}.png'\n",
    "fig_save_file4 = fig_save_file4.format(figure_dir, analogue_var, forecast_var, chosen_target_domain, chosen_target_region, chosen_window, chosen_num_mem, chosen_norm_window, smoothing_string, '', '', rmse_string, method_string, picontrols_string, hist_string, strong_string, ltbc_string, clim_string, stop_string)\n",
    "\n",
    "# ===============\n",
    "# This is the key function. Here we bias correct the analogue members,\n",
    "# which can be done in a million different ways\n",
    "# ===============\n",
    "trend_forecast_recentred = recentre_forecast(trend_forecast, chosen_num_mem, trend_forecast_means, trend_forecast_sds, forecast_obs, chosen_window,\n",
    "                                             old_recentre_method=old_recentre_method, new_recentre_method=new_recentre_method,\n",
    "                                             new_recentre_method2=new_recentre_method2, new_recentre_method2b=new_recentre_method2b,\n",
    "                                             simple_recentre_method=simple_recentre_method, simpleScaled_recentre_method=simpleScaled_recentre_method,\n",
    "                                             clever_scaling_method=clever_scaling_method, clever_scaling_methodb=clever_scaling_methodb,\n",
    "                                             clever_scaling_methodc=clever_scaling_methodc, map_method=map_method, map_method_nosd=map_method_nosd, chosen_norm_window=chosen_norm_window)\n",
    "trend_forecast_recentred_keepallmems = recentre_forecast(trend_forecast, chosen_num_mem, trend_forecast_means, trend_forecast_sds, forecast_obs, chosen_window,\n",
    "                                                         keep_mems=True, old_recentre_method=old_recentre_method, new_recentre_method=new_recentre_method,\n",
    "                                                         new_recentre_method2=new_recentre_method2, new_recentre_method2b=new_recentre_method2b,\n",
    "                                                         simple_recentre_method=simple_recentre_method, simpleScaled_recentre_method=simpleScaled_recentre_method,\n",
    "                                                         clever_scaling_method=clever_scaling_method, clever_scaling_methodb=clever_scaling_methodb,\n",
    "                                                         clever_scaling_methodc=clever_scaling_methodc, map_method=map_method, map_method_nosd=map_method_nosd, chosen_norm_window=chosen_norm_window)\n",
    "\n",
    "if do_ltbc:\n",
    "    drift = np.ma.mean(trend_forecast_recentred, axis=0)\n",
    "    drift -= drift[0]\n",
    "    for iyr in range(nyrs):\n",
    "        trend_forecast_recentred[iyr, :] -= drift\n",
    "        for imem in range(chosen_num_mem):\n",
    "            trend_forecast_recentred_keepallmems[iyr, imem, :] -= drift\n",
    "            \n",
    "# Lead time 2-10 combined\n",
    "trend_forecast_recentred_lt2t10 = np.ma.mean(trend_forecast_recentred[:, 2:11], axis=1)\n",
    "\n",
    "# Recalculate skill based on the new number of members\n",
    "trend_forecast_skill = calculate_skill(trend_forecast_recentred, nlead, forecast_obs, year_forecast_obs)\n",
    "trend_forecast_skill1960 = calculate_skill(trend_forecast_recentred, nlead, forecast_obs, year_forecast_obs, since1960=True)\n",
    "trend_forecast_skill1990 = calculate_skill(trend_forecast_recentred, nlead, forecast_obs, year_forecast_obs, before1990=True)\n",
    "trend_forecast_multiskill = calculate_skill(trend_forecast_recentred, nlead, forecast_obs, year_forecast_obs, multi=True,\n",
    "                                            start_lead=start_lead, end_lead=end_lead)\n",
    "trend_forecast_multiskill1960 = calculate_skill(trend_forecast_recentred, nlead, forecast_obs, year_forecast_obs, since1960=True,\n",
    "                                                multi=True, start_lead=start_lead, end_lead=end_lead)\n",
    "trend_forecast_multiskill1990 = calculate_skill(trend_forecast_recentred, nlead, forecast_obs, year_forecast_obs, before1990=True,\n",
    "                                                multi=True, start_lead=start_lead, end_lead=end_lead)\n",
    "\n",
    "trend_forecast_skill_running_lt1 = simple_running_skill(trend_forecast_recentred, running_skill_window, forecast_obs, ilead=[1])\n",
    "trend_forecast_skill_running_lt210 = simple_running_skill(trend_forecast_recentred, running_skill_window, forecast_obs, ilead=[2, 10])\n",
    "\n",
    "with open(baseline_save_file, 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "    historical_mmm = data['historical_mmm']\n",
    "    hindcast_time_series_ensmn_ltbc_mmm = data['hindcast_time_series_ensmn_ltbc_mmm']\n",
    "    hindcast_time_series_ensmn_ltbc_mm40m_lt2t10 = np.ma.mean(hindcast_time_series_ensmn_ltbc_mmm[:,2:11], axis=1)\n",
    "#    hindcast_time_series_ensmn_ltbc_mm40m_lt2t10 = data['hindcast_time_series_ensmn_ltbc_mm40m_lt2t10']\n",
    "trend_forecast_recentred_res = calculate_residual(trend_forecast_recentred, nlead, historical_mmm)\n",
    "forecast_obs_res = calculate_residual(forecast_obs, 1, historical_mmm)\n",
    "\n",
    "trend_forecast_res_skill = calculate_skill(trend_forecast_recentred_res, nlead, forecast_obs_res, year_forecast_obs, tol=0.1)\n",
    "trend_forecast_res_multiskill = calculate_skill(trend_forecast_recentred_res, nlead, forecast_obs_res, year_forecast_obs,\n",
    "                                                tol=0.1, multi=True, start_lead=start_lead, end_lead=end_lead)\n",
    "\n",
    "# ===============\n",
    "# We can probably remove the ann_forecast stuff but I have left it in for now\n",
    "# ===============\n",
    "ann_forecast_recentred = recentre_forecast(ann_forecast, chosen_num_mem, ann_forecast_means, ann_forecast_sds, forecast_obs, chosen_window,\n",
    "                                           old_recentre_method=old_recentre_method, new_recentre_method=new_recentre_method,\n",
    "                                           new_recentre_method2=new_recentre_method2, new_recentre_method2b=new_recentre_method2b,\n",
    "                                           simple_recentre_method=simple_recentre_method, simpleScaled_recentre_method=simpleScaled_recentre_method,\n",
    "                                           clever_scaling_method=clever_scaling_method, clever_scaling_methodb=clever_scaling_methodb,\n",
    "                                           clever_scaling_methodc=clever_scaling_methodc, map_method=map_method, map_method_nosd=map_method_nosd, chosen_norm_window=chosen_norm_window)\n",
    "ann_forecast_recentred_keepallmems = recentre_forecast(ann_forecast, chosen_num_mem, ann_forecast_means, ann_forecast_sds, forecast_obs, chosen_window,\n",
    "                                                       keep_mems=True, old_recentre_method=old_recentre_method, new_recentre_method=new_recentre_method,\n",
    "                                                       new_recentre_method2=new_recentre_method2, new_recentre_method2b=new_recentre_method2b,\n",
    "                                                       simple_recentre_method=simple_recentre_method, simpleScaled_recentre_method=simpleScaled_recentre_method,\n",
    "                                                       clever_scaling_method=clever_scaling_method, clever_scaling_methodb=clever_scaling_methodb,\n",
    "                                                       clever_scaling_methodc=clever_scaling_methodc, map_method=map_method, map_method_nosd=map_method_nosd, chosen_norm_window=chosen_norm_window)\n",
    "\n",
    "if do_ltbc:\n",
    "    drift = np.ma.mean(ann_forecast_recentred, axis=0)\n",
    "    drift -= drift[0]\n",
    "    for iyr in range(nyrs):\n",
    "        ann_forecast_recentred[iyr, :] -= drift\n",
    "        for imem in range(chosen_num_mem):\n",
    "            ann_forecast_recentred_keepallmems[iyr, imem, :] -= drift\n",
    "\n",
    "# Lead time 2-10 combined\n",
    "ann_forecast_recentred_lt2t10 = np.ma.mean(ann_forecast_recentred[:, 2:11], axis=1)\n",
    "\n",
    "# Recalculate skill based on the new number of members\n",
    "ann_forecast_skill = calculate_skill(ann_forecast_recentred, nlead, forecast_obs, year_forecast_obs)\n",
    "ann_forecast_skill1960 = calculate_skill(ann_forecast_recentred, nlead, forecast_obs, year_forecast_obs, since1960=True)\n",
    "ann_forecast_skill1990 = calculate_skill(ann_forecast_recentred, nlead, forecast_obs, year_forecast_obs, before1990=True)\n",
    "ann_forecast_multiskill = calculate_skill(ann_forecast_recentred, nlead, forecast_obs, year_forecast_obs, multi=True,\n",
    "                                          start_lead=start_lead, end_lead=end_lead)\n",
    "ann_forecast_multiskill1960 = calculate_skill(ann_forecast_recentred, nlead, forecast_obs, year_forecast_obs, since1960=True,\n",
    "                                              multi=True, start_lead=start_lead, end_lead=end_lead)\n",
    "ann_forecast_multiskill1990 = calculate_skill(ann_forecast_recentred, nlead, forecast_obs, year_forecast_obs, before1990=True,\n",
    "                                              multi=True, start_lead=start_lead, end_lead=end_lead)\n",
    "\n",
    "ann_forecast_skill_running_lt1 = simple_running_skill(ann_forecast_recentred, running_skill_window, forecast_obs, ilead=[1])\n",
    "ann_forecast_skill_running_lt210 = simple_running_skill(ann_forecast_recentred, running_skill_window, forecast_obs, ilead=[2, 10])\n",
    "\n",
    "# with open(baseline_save_file, 'rb') as handle:\n",
    "#     historical_mmm = pickle.load(handle)['historical_mmm']\n",
    "ann_forecast_recentred_res = calculate_residual(ann_forecast_recentred, nlead, historical_mmm)\n",
    "forecast_obs_res = calculate_residual(forecast_obs, 1, historical_mmm)\n",
    "\n",
    "ann_forecast_res_skill = calculate_skill(ann_forecast_recentred_res, nlead, forecast_obs_res, year_forecast_obs, tol=0.1)\n",
    "ann_forecast_res_multiskill = calculate_skill(ann_forecast_recentred_res, nlead, forecast_obs_res, year_forecast_obs,\n",
    "                                              tol=0.1, multi=True, start_lead=start_lead, end_lead=end_lead)\n",
    "\n",
    "# Skill of random forecast (shows how much info comes from skill/recentre method)\n",
    "# random_forecast = np.ma.array(np.random.random(trend_forecast.shape), mask=trend_forecast.mask)\n",
    "random_forecast = ann_forecast.copy()\n",
    "random_forecast_means = ann_forecast_means.copy()\n",
    "random_forecast_sds = ann_forecast_sds.copy()\n",
    "random_corr_info = ann_corr_info.copy()\n",
    "\n",
    "nt, nm, _ = ann_forecast.shape\n",
    "non_miss_ind = np.nonzero(ann_forecast[:, :, 0].flatten())[0]\n",
    "np.random.shuffle(non_miss_ind) ## Randomise!\n",
    "count = 0\n",
    "for tt in range(nt):\n",
    "    for mm in range(nm):\n",
    "        if np.ma.is_masked(ann_forecast[tt, mm, 0]):\n",
    "            continue\n",
    "        tt2, mm2 = np.unravel_index(non_miss_ind[count], ann_forecast[:, :, 0].shape)\n",
    "        random_forecast[tt, mm, :] = ann_forecast[tt2, mm2, :].copy()\n",
    "        random_forecast_means[tt, mm] = ann_forecast_means[tt2, mm2].copy()\n",
    "        random_forecast_sds[tt, mm] = ann_forecast_sds[tt2, mm2].copy()\n",
    "        random_corr_info[tt, mm, :] = ann_corr_info[tt2, mm2, :].copy()\n",
    "        count += 1\n",
    "        \n",
    "# ===============\n",
    "# An easy way to test the skill of our analogue system is to just randomise its order and then apply the same bias corrections.\n",
    "# If our system is truly skilful then it should be able to beat this. If not, then all the skill comes from the method of\n",
    "# bias correction, which is essentially re-adding the observed long-period variability.\n",
    "# ===============\n",
    "random_forecast_recentred = recentre_forecast(random_forecast, chosen_num_mem, random_forecast_means, random_forecast_sds, forecast_obs, chosen_window,\n",
    "                                              old_recentre_method=old_recentre_method, new_recentre_method=new_recentre_method,\n",
    "                                              new_recentre_method2=new_recentre_method2, new_recentre_method2b=new_recentre_method2b,\n",
    "                                              simple_recentre_method=simple_recentre_method, simpleScaled_recentre_method=simpleScaled_recentre_method,\n",
    "                                              clever_scaling_method=clever_scaling_method, clever_scaling_methodb=clever_scaling_methodb,\n",
    "                                              clever_scaling_methodc=clever_scaling_methodc, map_method=map_method, map_method_nosd=map_method_nosd, chosen_norm_window=chosen_norm_window)\n",
    "random_forecast_skill = calculate_skill(random_forecast_recentred, nlead, forecast_obs, year_forecast_obs)\n",
    "random_forecast_skill1960 = calculate_skill(random_forecast_recentred, nlead, forecast_obs, year_forecast_obs, since1960=True)\n",
    "random_forecast_skill1990 = calculate_skill(random_forecast_recentred, nlead, forecast_obs, year_forecast_obs, before1990=True)\n",
    "random_forecast_multiskill = calculate_skill(random_forecast_recentred, nlead, forecast_obs, year_forecast_obs, multi=True,\n",
    "                                             start_lead=start_lead, end_lead=end_lead)\n",
    "random_forecast_multiskill1960 = calculate_skill(random_forecast_recentred, nlead, forecast_obs, year_forecast_obs, multi=True,\n",
    "                                                 start_lead=start_lead, end_lead=end_lead, since1960=True)\n",
    "random_forecast_multiskill1990 = calculate_skill(random_forecast_recentred, nlead, forecast_obs, year_forecast_obs, multi=True,\n",
    "                                                 start_lead=start_lead, end_lead=end_lead, before1990=True)\n",
    "\n",
    "# # Write the output files list (so we can easily find the raw data associated with these plots later)\n",
    "# analogue_files_list = get_raw_files(trend_corr_info, chosen_num_mem, analogue_var)\n",
    "# with open(analogue_output_filelist, 'wb') as handle:\n",
    "#     print \"Writing ANALOGUE source file list to {:s}\".format(analogue_output_filelist)\n",
    "#     pickle.dump(analogue_files_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# forecast_files_list = get_raw_files(trend_corr_info, chosen_num_mem, forecast_var)\n",
    "# with open(forecast_output_filelist, 'wb') as handle:\n",
    "#     print \"Writing FORECAST source file list to {:s}\".format(forecast_output_filelist)\n",
    "#     pickle.dump(forecast_files_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 16\n",
    "lead_indices_to_plot = [0, 1, 2, 5, 10]\n",
    "color = ['red', 'blue', 'green', 'orange', 'grey']\n",
    "target_ts_norm = (forecast_obs - forecast_obs.mean()) * (1. / forecast_obs.std())\n",
    "expt_colors = ['k',  'orange', 'green', 'blue', 'yellow', 'cyan', 'red', 'red',\n",
    "               'purple', 'purple'] + ['grey'] * 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_slow_plots:\n",
    "    gs1 = gridspec.GridSpec(5, 1)\n",
    "    gs1.update(wspace=0.2)\n",
    "\n",
    "    title1 = 'Analogue input data: Spatial corr (at max) (TREND over last {:d} years)'.format(chosen_window)\n",
    "    corr_info = trend_corr_info[:, max_mems_to_take-chosen_num_mem:, :].copy()\n",
    "    method = 'Trend'\n",
    "\n",
    "    # =================\n",
    "    # Plot the analogue info to check it is sensible\n",
    "    # =================\n",
    "    plt.figure(figsize=(15, 25))\n",
    "    this_ax = plt.subplot(gs1[0, 0])\n",
    "    for ii in range(chosen_num_mem):\n",
    "        if ii == 0:\n",
    "            if rmse_method:\n",
    "                label = 'Spatial 1/RMSE in analogue window ({:d} years)'.format(chosen_window)\n",
    "            else:\n",
    "                label = 'Spatial corr. in analogue window ({:d} years)'.format(chosen_window)\n",
    "        else:\n",
    "            label = None\n",
    "        if rmse_method == True:\n",
    "            scaling = 1. / np.ma.max(np.ma.masked_invalid(np.ma.asarray(corr_info[chosen_window-1:, :chosen_num_mem, 0].data, dtype=float)))\n",
    "            plt.plot(year_forecast_obs[chosen_window-1:], np.ma.asarray(corr_info[chosen_window-1:, ii, 0].data, dtype=float) * scaling, 'x', color='red', label=label)\n",
    "            if ii == 0:\n",
    "                plt.plot(year_forecast_obs, 0.5 + target_ts_norm / 5., color='k', label='Obs (TRUTH) scaled')\n",
    "        else:\n",
    "            corr_info_mean = corr_info[:, 0, 0].mean()\n",
    "            plt.plot(year_forecast_obs, corr_info[:, ii, 0].data, 'x', color='red', label=label)\n",
    "            ylims = plt.gca().get_ylim()\n",
    "            if ii == 0:\n",
    "                plt.plot(year_forecast_obs, corr_info_mean + target_ts_norm * (ylims[1] - ylims[0]) / 5., color='k', label='Obs (TRUTH) scaled')\n",
    "    plt.plot(year_forecast_obs, trend_forecast_skill_running_lt1, color='blue', label='TREND Running skill (LT=1) over last {:d} years'.format(running_skill_window))\n",
    "    plt.plot(year_forecast_obs, trend_forecast_skill_running_lt210, linestyle='--', color='blue', label='TREND Running skill (LT=2-10) over last {:d} years'.format(running_skill_window))\n",
    "    plt.plot(year_forecast_obs, ann_forecast_skill_running_lt1, color='indigo', label='ANN Running skill (LT=1) over last {:d} years'.format(running_skill_window))\n",
    "    plt.plot(year_forecast_obs, ann_forecast_skill_running_lt210, linestyle='--', color='indigo', label='ANN Running skill (LT=2-10) over last {:d} years'.format(running_skill_window))\n",
    "    plt.title(title1, fontsize=fontsize)\n",
    "    plt.xticks([])\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.ylabel('Correlation', fontsize=fontsize)\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(loc=0, ncol=1)\n",
    "\n",
    "    colors = plt.cm.Reds_r(np.linspace(0., 0.75, chosen_num_mem))\n",
    "    this_ax = plt.subplot(gs1[1, 0])\n",
    "    for iyr, year in enumerate(year_forecast_obs):\n",
    "        for imem in range(chosen_num_mem):\n",
    "            if year == 2000:\n",
    "                label = 'Member #'.format(imem)\n",
    "                if imem == 0:\n",
    "                    label += ' (Best)'\n",
    "            else:\n",
    "                label=None\n",
    "    #                                 plt.plot(year+np.arange(nlead), trend_forecast[iyr, -1-imem, :], color=colors[imem], label=label)\n",
    "            plt.plot(year+np.arange(nlead), trend_forecast_recentred_keepallmems[iyr, -1-imem, :], color=colors[imem], label=label)\n",
    "    plt.plot(year_forecast_obs, forecast_obs, color='k', label='Obs (TRUTH)')\n",
    "    plt.plot(year_forecast_obs, random_forecast_recentred[:, 0], color='orange', label='Noise+RecentreMethod')\n",
    "    ylim_plot = np.array(plt.gca().get_ylim())\n",
    "    scaled = (((forecast_obs - forecast_obs.mean()) / forecast_obs.ptp()) * ylim_plot.ptp() + ylim_plot.mean())\n",
    "    plt.plot(year_forecast_obs, scaled, color='grey', label='Obs (TRUTH) (scaled)')\n",
    "    plt.legend(loc=0, ncol=2)\n",
    "    plt.title('Raw forecast data and obs (truth)'.format(chosen_window), fontsize=fontsize)\n",
    "    plt.xticks([], fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.ylabel(forecast_var, fontsize=fontsize)\n",
    "    plt.xlim(xlim)\n",
    "\n",
    "    this_ax = plt.subplot(gs1[2, 0])\n",
    "    for ii, ilead in enumerate(lead_indices_to_plot):\n",
    "        plt.plot(year_forecast_obs+ilead, trend_forecast_recentred[:, ilead],\n",
    "                 label='Analogue ({:s}-based) (lead={:d}y)'.format(method, ilead), color=color[ii])\n",
    "    plt.plot(year_forecast_obs, forecast_obs, color='k', label='Obs (TRUTH)')\n",
    "    ii = nyrs - 1 - 1  # NOT SURE why 2018 is missing\n",
    "    plt.plot(year_forecast_obs[ii]+np.arange(nlead), trend_forecast_recentred[ii, :], color='k', linestyle='--', label='Real future forecast')\n",
    "    plt.legend(loc=0, ncol=2)\n",
    "    plt.title('Time series in analogue and obs (truth)'.format(chosen_window), fontsize=fontsize)\n",
    "    plt.xticks([], fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.ylabel(forecast_var, fontsize=fontsize)\n",
    "    plt.xlim(xlim)\n",
    "\n",
    "    model_matrix, expt_matrix, models, expts, expts_used = make_info_matrices(corr_info, year_forecast_obs, chosen_num_mem)\n",
    "    #                         print models\n",
    "    models_padded = models[:]\n",
    "    for jj, model in enumerate(models):\n",
    "    #                             print jj, model\n",
    "        if (jj % 2) == 0:\n",
    "            models_padded[jj] = model + ' ' * 25\n",
    "\n",
    "    this_ax = plt.subplot(gs1[3:, 0])\n",
    "    for jj in range(chosen_num_mem):\n",
    "        for iyr, year in enumerate(year_forecast_obs):\n",
    "            model_num = model_matrix[iyr, jj]\n",
    "            if (model_num % 3) == 0:\n",
    "                marker = 'x'\n",
    "            elif (model_num % 3) == 1:\n",
    "                marker = 'o'\n",
    "            elif (model_num % 3) == 2:\n",
    "                marker = '^'\n",
    "            else:\n",
    "                continue  # If masked\n",
    "            plt.plot(year_forecast_obs[iyr], model_num, marker, color=expt_colors[expt_matrix[iyr, jj]],\n",
    "                     markerfacecolor='None', mew=2, markersize=7)\n",
    "    plt.xlabel('Year', fontsize=fontsize)\n",
    "    plt.xticks(fontsize=fontsize)\n",
    "    plt.yticks(np.arange(len(models)))\n",
    "    plt.gca().set_yticklabels(models_padded, fontsize=10)\n",
    "    plt.title('{:s} method: The model(s) and experiment(s) used in each window'.format(method), fontsize=fontsize)\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(-1, len(models))\n",
    "    for num in range(0, len(models), 3):\n",
    "        plt.axhline(num, linestyle=':', color='grey')\n",
    "\n",
    "    custom_lines = []\n",
    "    for jj, this_expt in enumerate(expts):\n",
    "        if this_expt not in expts_used:\n",
    "            continue\n",
    "        custom_lines.append(Line2D([0], [0], color=expt_colors[jj], lw=2))\n",
    "    plt.legend(custom_lines, expts_used, loc=3, fontsize=fontsize, handlelength=1.2)\n",
    "\n",
    "    print fig_save_file1\n",
    "    plt.savefig(fig_save_file1, bbox_inches = 'tight')\n",
    "    #                         plt.close(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_slow_plots:\n",
    "    gs1 = gridspec.GridSpec(5, 1)\n",
    "    gs1.update(wspace=0.2)\n",
    "\n",
    "    title1 = 'Analogue input data: Spatial corr (at max) (MEAN over last {:d} years)'.format(chosen_window)\n",
    "    corr_info = ann_corr_info[:, max_mems_to_take-chosen_num_mem:, :].copy()\n",
    "    method = 'ann'\n",
    "\n",
    "    # =================\n",
    "    # Plot the analogue info to check it is sensible\n",
    "    # =================\n",
    "    plt.figure(figsize=(15, 25))\n",
    "    this_ax = plt.subplot(gs1[0, 0])\n",
    "    for ii in range(chosen_num_mem):\n",
    "        if ii == 0:\n",
    "            if rmse_method:\n",
    "                label = 'Spatial 1/RMSE in analogue window ({:d} years)'.format(chosen_window)\n",
    "            else:\n",
    "                label = 'Spatial corr. in analogue window ({:d} years)'.format(chosen_window)\n",
    "        else:\n",
    "            label = None\n",
    "        if rmse_method == True:\n",
    "            scaling = 1. / np.ma.max(np.ma.masked_invalid(np.ma.asarray(corr_info[chosen_window-1:, :chosen_num_mem, 0].data, dtype=float)))\n",
    "            plt.plot(year_forecast_obs[chosen_window-1:], np.ma.asarray(corr_info[chosen_window-1:, ii, 0].data, dtype=float) * scaling, 'x', color='red', label=label)\n",
    "            if ii == 0:\n",
    "                plt.plot(year_forecast_obs, 0.5 + target_ts_norm / 5., color='k', label='Obs (TRUTH) scaled')\n",
    "        else:\n",
    "            corr_info_mean = corr_info[:, 0, 0].mean()\n",
    "            plt.plot(year_forecast_obs, corr_info[:, ii, 0].data, 'x', color='red', label=label)\n",
    "            ylims = plt.gca().get_ylim()\n",
    "            if ii == 0:\n",
    "                plt.plot(year_forecast_obs, corr_info_mean + target_ts_norm * (ylims[1] - ylims[0]) / 5., color='k', label='Obs (TRUTH) scaled')\n",
    "    plt.plot(year_forecast_obs, trend_forecast_skill_running_lt1, color='blue', label='TREND Running skill (LT=1) over last {:d} years'.format(running_skill_window))\n",
    "    plt.plot(year_forecast_obs, trend_forecast_skill_running_lt210, linestyle='--', color='blue', label='TREND Running skill (LT=2-10) over last {:d} years'.format(running_skill_window))\n",
    "    plt.plot(year_forecast_obs, ann_forecast_skill_running_lt1, color='indigo', label='ANN Running skill (LT=1) over last {:d} years'.format(running_skill_window))\n",
    "    plt.plot(year_forecast_obs, ann_forecast_skill_running_lt210, linestyle='--', color='indigo', label='ANN Running skill (LT=2-10) over last {:d} years'.format(running_skill_window))\n",
    "    plt.title(title1, fontsize=fontsize)\n",
    "    plt.xticks([])\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.ylabel('Correlation', fontsize=fontsize)\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(loc=0, ncol=1)\n",
    "\n",
    "    colors = plt.cm.Reds_r(np.linspace(0., 0.75, chosen_num_mem))\n",
    "    this_ax = plt.subplot(gs1[1, 0])\n",
    "    for iyr, year in enumerate(year_forecast_obs):\n",
    "        for imem in range(chosen_num_mem):\n",
    "            if year == 2000:\n",
    "                label = 'Member #'.format(imem)\n",
    "                if imem == 0:\n",
    "                    label += ' (Best)'\n",
    "            else:\n",
    "                label=None\n",
    "    #                                 plt.plot(year+np.arange(nlead), ann_forecast[iyr, -1-imem, :], color=colors[imem], label=label)\n",
    "            plt.plot(year+np.arange(nlead), ann_forecast_recentred_keepallmems[iyr, -1-imem, :], color=colors[imem], label=label)\n",
    "    plt.plot(year_forecast_obs, forecast_obs, color='k', label='Obs (TRUTH)')\n",
    "    plt.plot(year_forecast_obs, random_forecast_recentred[:, 0], color='orange', label='Noise+RecentreMethod')\n",
    "    ylim_plot = np.array(plt.gca().get_ylim())\n",
    "    scaled = (((forecast_obs - forecast_obs.mean()) / forecast_obs.ptp()) * ylim_plot.ptp() + ylim_plot.mean())\n",
    "    plt.plot(year_forecast_obs, scaled, color='grey', label='Obs (TRUTH) (scaled)')\n",
    "    plt.legend(loc=0, ncol=2)\n",
    "    plt.title('Raw forecast data and obs (truth)'.format(chosen_window), fontsize=fontsize)\n",
    "    plt.xticks([], fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.ylabel(forecast_var, fontsize=fontsize)\n",
    "    plt.xlim(xlim)\n",
    "\n",
    "    this_ax = plt.subplot(gs1[2, 0])\n",
    "    for ii, ilead in enumerate(lead_indices_to_plot):\n",
    "        plt.plot(year_forecast_obs+ilead, ann_forecast_recentred[:, ilead],\n",
    "                 label='Analogue ({:s}-based) (lead={:d}y)'.format(method, ilead), color=color[ii])\n",
    "    plt.plot(year_forecast_obs, forecast_obs, color='k', label='Obs (TRUTH)')\n",
    "    ii = nyrs - 1 - 1  # NOT SURE why 2018 is missing\n",
    "    plt.plot(year_forecast_obs[ii]+np.arange(nlead), ann_forecast_recentred[ii, :], color='k', linestyle='--', label='Real future forecast')\n",
    "    plt.legend(loc=0, ncol=2)\n",
    "    plt.title('Time series in analogue and obs (truth)'.format(chosen_window), fontsize=fontsize)\n",
    "    plt.xticks([], fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.ylabel(forecast_var, fontsize=fontsize)\n",
    "    plt.xlim(xlim)\n",
    "\n",
    "    model_matrix, expt_matrix, models, expts, expts_used = make_info_matrices(corr_info, year_forecast_obs, chosen_num_mem)\n",
    "    #                         print models\n",
    "    models_padded = models[:]\n",
    "    for jj, model in enumerate(models):\n",
    "    #                             print jj, model\n",
    "        if (jj % 2) == 0:\n",
    "            models_padded[jj] = model + ' ' * 25\n",
    "\n",
    "    this_ax = plt.subplot(gs1[3:, 0])\n",
    "    for jj in range(chosen_num_mem):\n",
    "        for iyr, year in enumerate(year_forecast_obs):\n",
    "            model_num = model_matrix[iyr, jj]\n",
    "            if (model_num % 3) == 0:\n",
    "                marker = 'x'\n",
    "            elif (model_num % 3) == 1:\n",
    "                marker = 'o'\n",
    "            elif (model_num % 3) == 2:\n",
    "                marker = '^'\n",
    "            else:\n",
    "                continue  # If masked\n",
    "            plt.plot(year_forecast_obs[iyr], model_num, marker, color=expt_colors[expt_matrix[iyr, jj]],\n",
    "                     markerfacecolor='None', mew=2, markersize=7)\n",
    "    plt.xlabel('Year', fontsize=fontsize)\n",
    "    plt.xticks(fontsize=fontsize)\n",
    "    plt.yticks(np.arange(len(models)))\n",
    "    plt.gca().set_yticklabels(models_padded, fontsize=10)\n",
    "    plt.title('{:s} method: The model(s) and experiment(s) used in each window'.format(method), fontsize=fontsize)\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(-1, len(models))\n",
    "    for num in range(0, len(models), 3):\n",
    "        plt.axhline(num, linestyle=':', color='grey')\n",
    "\n",
    "    custom_lines = []\n",
    "    for jj, this_expt in enumerate(expts):\n",
    "        if this_expt not in expts_used:\n",
    "            continue\n",
    "        custom_lines.append(Line2D([0], [0], color=expt_colors[jj], lw=2))\n",
    "    plt.legend(custom_lines, expts_used, loc=3, fontsize=fontsize, handlelength=1.2)\n",
    "\n",
    "    print fig_save_file1b\n",
    "    plt.savefig(fig_save_file1b, bbox_inches = 'tight')\n",
    "    #                         plt.close(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we show the various steps that lead to the forecast more clearly\n",
    "# print year_forecast_obs.shape, ann_forecast.shape\n",
    "\n",
    "# year_reshaped = np.concatenate((year_forecast_obs, year_forecast_obs[-1]+np.arange(nlead-1)+1))  # Add the future!\n",
    "# yvalues = np.linspace(3, 11.5, 69)\n",
    "# year_mesh, ann_forecast_mesh = np.meshgrid(year_reshaped, yvalues)\n",
    "# mesh = np.vstack([year_mesh.ravel(), ann_forecast_mesh.ravel()])\n",
    "\n",
    "# ====================\n",
    "# Decided against KDE as it smooths in X too, which I don't want\n",
    "# ====================\n",
    "# # Now make the KDEs\n",
    "# def make_kde(year_in, ann_forecast_in):\n",
    "#     nyrs2 = nyrs + nlead - 1\n",
    "#     ann_forecast_overlaid = np.ma.masked_all(shape=(nyrs2, chosen_num_mem * nlead))\n",
    "#     year_reshaped = np.concatenate((year_in, year_forecast_obs[-1]+np.arange(nlead-1)+1))  # Add the future!\n",
    "#     year_reshaped = np.repeat(year_reshaped[:, np.newaxis], chosen_num_mem * nlead, axis=1)\n",
    "#     print year_reshaped.shape, ann_forecast_overlaid.shape\n",
    "#     for ilead in range(nlead):\n",
    "#         ann_forecast_overlaid[ilead:nyrs2-((nlead-1)-ilead), ilead*chosen_num_mem:(ilead+1)*chosen_num_mem] = ann_forecast_in[:, :, ilead].copy()\n",
    "#     iyr, ilead, imem = 120, 3, 30\n",
    "#     assert ann_forecast_overlaid[iyr+ilead, (ilead*chosen_num_mem)+imem] == ann_forecast_in[iyr, imem, ilead]\n",
    "#     ind = ~ann_forecast_overlaid.mask\n",
    "#     # fit an array of size [Ndim, Nsamples]\n",
    "#     stacked = np.vstack([year_reshaped[ind], ann_forecast_overlaid[ind]])\n",
    "#     return gaussian_kde(stacked)\n",
    "\n",
    "# ann_forecast_kde = make_kde(year_forecast_obs, ann_forecast)\n",
    "# ann_forecast_recentred_kde = make_kde(year_forecast_obs, ann_forecast_recentred_keepallmems)\n",
    "\n",
    "# print \"Evaluating KDE\"\n",
    "# ann_forecast_kde_contour = ann_forecast_kde.evaluate(mesh).reshape(year_mesh.shape)\n",
    "\n",
    "# print \"Evaluating KDE\"\n",
    "# ann_forecast_recentred_kde_contour = ann_forecast_recentred_kde.evaluate(mesh).reshape(year_mesh.shape)\n",
    "\n",
    "# mask = np.empty(shape=ann_forecast_recentred_kde_contour.shape, dtype='bool')\n",
    "# mask[:] = False\n",
    "# mask[:, :chosen_window-1] = True\n",
    "\n",
    "# ann_forecast_kde_contour = np.ma.array(ann_forecast_kde_contour, mask=mask)\n",
    "# ann_forecast_recentred_kde_contour = np.ma.array(ann_forecast_recentred_kde_contour, mask=mask)\n",
    "\n",
    "def make_density(in_arr_3d, edges):\n",
    "    nyrs2 = nyrs + nlead - 1\n",
    "    ann_forecast_overlaid = np.ma.masked_all(shape=(nyrs2, chosen_num_mem * nlead))\n",
    "    for ilead in range(nlead):\n",
    "        if ilead == 0: continue\n",
    "#         print ilead, nyrs2-((nlead-1)-ilead), ilead*chosen_num_mem, (ilead+1)*chosen_num_mem\n",
    "        ann_forecast_overlaid[ilead:nyrs2-((nlead-1)-ilead), ilead*chosen_num_mem:(ilead+1)*chosen_num_mem] = in_arr_3d[:, :, ilead].copy()\n",
    "    \n",
    "    density = np.ma.masked_all(shape=(len(edges)-1, nyrs2))\n",
    "    for iyr in range(nyrs2):\n",
    "        this_slice = ann_forecast_overlaid[iyr, :]\n",
    "        reals = np.ma.nonzero(this_slice)\n",
    "#         print iyr, len(reals[0])\n",
    "        if len(reals[0]) < 5: continue\n",
    "        hist_s, e1 = np.histogram(this_slice[reals], bins=edges)\n",
    "        density[:, iyr] = hist_s\n",
    "#         print np.ma.sum(hist_s)\n",
    "    return density\n",
    "\n",
    "year_reshaped = np.concatenate((year_forecast_obs, year_forecast_obs[-1]+np.arange(nlead-1)+1))  # Add the future!\n",
    "year_reshaped_edges = np.concatenate((year_forecast_obs, year_forecast_obs[-1]+np.arange(nlead)+1)) - 0.5\n",
    "edges = np.linspace(3, 11.5, 35)\n",
    "mids = (edges[1:] + edges[:-1]) / 2.\n",
    "\n",
    "ann_forecast_kde_contour = make_density(ann_forecast, edges)\n",
    "\n",
    "t0 = np.argwhere(year_forecast_obs == 1960)[0][0]\n",
    "t1 = np.argwhere(year_forecast_obs == 1990)[0][0]\n",
    "\n",
    "obs_time_series_mean = np.ma.mean(forecast_obs[t0:t1])\n",
    "obs_time_series_sd = np.ma.std(forecast_obs[t0:t1])\n",
    "\n",
    "allmemsclimmean = np.ma.mean(ann_forecast_recentred_keepallmems[t0:t1, :, :], axis=0, keepdims=True)\n",
    "allmemsclimsd = np.ma.std(ann_forecast_recentred_keepallmems[t0:t1, :, :], axis=0, keepdims=True)\n",
    "\n",
    "# This is normalising by the climatology period - not what we want!\n",
    "# ann_forecast_recentred_keepallmems_anomadjusted = (ann_forecast_recentred_keepallmems - allmemsclimmean) * (obs_time_series_sd / allmemsclimsd) + obs_time_series_mean\n",
    "\n",
    "# This is anomaly adjusting the pre-normalised (using /keep_all_mems) analogue forecast.\n",
    "# However, note that /keep_all_mems doesn't quite do what we want (see analogue.py) as normalising by the MMM\n",
    "# results in crazy values for _individual_ ensemble members, and there is no sensible way to fix it\n",
    "ann_forecast_recentred_keepallmems_anomadjusted = (ann_forecast_recentred_keepallmems - allmemsclimmean) + obs_time_series_mean\n",
    "\n",
    "ann_forecast_recentred_kde_contour = make_density(ann_forecast_recentred_keepallmems_anomadjusted, edges)\n",
    "\n",
    "print ann_forecast_recentred_kde_contour.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_obs_lt210 = np.ma.masked_all(shape=nyrs)\n",
    "for iyr, year in enumerate(year_forecast_obs):\n",
    "    if (iyr+11) > nyrs:\n",
    "        if (iyr+11) == (nyrs+1):\n",
    "            forecast_obs_with_2020 = np.zeros(shape=(9))\n",
    "            forecast_obs_with_2020[:8] = forecast_obs[iyr+2:iyr+10]\n",
    "            forecast_obs_with_2020[8] = hadisst_spg_2020\n",
    "            forecast_obs_lt210_2020 = np.ma.mean(forecast_obs_with_2020)\n",
    "        continue\n",
    "    forecast_obs_lt210[iyr] = np.ma.mean(forecast_obs[iyr+2:iyr+11])\n",
    "year_forecast_obs_shifted = year_forecast_obs + 10\n",
    "year_forecast_obs_shifted_masked = np.ma.masked_less(year_forecast_obs_shifted, 1960)\n",
    "year_forecast_obs_shifted_masked = np.ma.masked_greater(year_forecast_obs_shifted_masked, 1990)\n",
    "\n",
    "ann_forecast_lt210 = np.ma.mean(ann_forecast_recentred[:, 2:11], axis=1)\n",
    "ind = np.nonzero(forecast_obs_lt210 * ann_forecast_lt210)\n",
    "time_series_mean = np.ma.mean(ann_forecast_lt210[ind])\n",
    "time_series_sd = np.ma.std(ann_forecast_lt210[ind])\n",
    "ann_forecast_lt210_scaled = (ann_forecast_lt210 - time_series_mean) * (np.ma.std(forecast_obs_lt210[ind]) / time_series_sd) + np.ma.mean(forecast_obs_lt210[ind])\n",
    "\n",
    "ind = np.nonzero(forecast_obs_lt210 * ann_forecast_lt210 * year_forecast_obs_shifted_masked)\n",
    "ann_forecast_lt210_anomadjust = (ann_forecast_lt210 - np.ma.mean(ann_forecast_lt210[ind])) + np.ma.mean(forecast_obs_lt210[ind])\n",
    "\n",
    "# levels_kde = np.linspace(0, 1.3e-1, num=21)\n",
    "# levels_kde2 = np.linspace(0, 3e-1, num=21)\n",
    "levels_kde = np.linspace(0, 1., num=21)\n",
    "levels_kde2 = levels_kde\n",
    "\n",
    "cmap_kde = plt.get_cmap('Greys')\n",
    "norm_kde = BoundaryNorm(levels_kde, ncolors=cmap_kde.N, clip=True)\n",
    "norm_kde2 = BoundaryNorm(levels_kde2, ncolors=cmap_kde.N, clip=True)\n",
    "\n",
    "xlim = (1870, 2028)\n",
    "# ylim = (yvalues[0], yvalues[-1])\n",
    "ylim = [7, 11]\n",
    "lw = 3\n",
    "ilev1, ilev2 = 5, 12\n",
    "textx, texty = 1872, 10.9\n",
    "textx2 = 1990\n",
    "mew = 2\n",
    "markersize = 10\n",
    "\n",
    "print levels_kde[ilev1], levels_kde[ilev2]\n",
    "\n",
    "ann_forecast_kde_contour_norm = ann_forecast_kde_contour / np.ma.sum(ann_forecast_kde_contour, axis=0)[np.newaxis, :]\n",
    "ann_forecast_recentred_kde_contour_norm = ann_forecast_recentred_kde_contour / np.ma.sum(ann_forecast_recentred_kde_contour, axis=0)[np.newaxis, :]\n",
    "cumul = np.cumsum(np.ma.array(ann_forecast_kde_contour_norm, fill_value=0.), axis=0)\n",
    "cumul_recentred = np.cumsum(np.ma.array(ann_forecast_recentred_kde_contour_norm, fill_value=0.), axis=0)\n",
    "\n",
    "# Overwrite with [0, 1] version\n",
    "ann_forecast_kde_contour_norm = ann_forecast_kde_contour / np.ma.max(ann_forecast_kde_contour, axis=0)[np.newaxis, :]\n",
    "ann_forecast_recentred_kde_contour_norm = ann_forecast_recentred_kde_contour / np.ma.max(ann_forecast_recentred_kde_contour, axis=0)[np.newaxis, :]\n",
    "\n",
    "gs1 = gridspec.GridSpec(2, 1)\n",
    "gs1.update(wspace=0.0, hspace=0.1)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "this_ax = plt.subplot(gs1[0, 0])\n",
    "pos1 = this_ax.get_position()\n",
    "clevs = plt.pcolormesh(year_reshaped_edges, edges, ann_forecast_kde_contour_norm, cmap=cmap_kde, norm=norm_kde, rasterized=True)\n",
    "# plt.contour(year_reshaped, yvalues2, cumul, levels=[0.1, 0.9], colors='grey', linewidths=3)\n",
    "# plt.contour(year_reshaped, yvalues2, cumul, levels=[0.5], colors='grey', linewidths=3)\n",
    "\n",
    "plt.plot(np.array([1920, 1920+chosen_window]), np.array([10.7, 10.7]), color='orange', lw=3)\n",
    "plt.plot(np.array([1920, 1920]), np.array([10.5, 10.9]), color='orange', lw=3)\n",
    "plt.plot(np.array([1920, 1920])+chosen_window, np.array([10.5, 10.9]), color='orange', lw=3)\n",
    "plt.text(1920+chosen_window/2., 10.6, \"Comparison\\nwindow\", fontsize=fontsize, color='orange', horizontalalignment='center', verticalalignment='top')\n",
    "\n",
    "plt.plot(np.array([1920+chosen_window, 1920+chosen_window+10]), np.array([10.7, 10.7]), color='blue', lw=3)\n",
    "plt.plot(np.array([1920, 1920])+chosen_window, np.array([10.5, 10.9]), color='blue', lw=3, linestyle=':')\n",
    "plt.plot(np.array([1920, 1920])+chosen_window+10, np.array([10.6, 10.9]), color='blue', lw=3)\n",
    "plt.text(1920+chosen_window+10/2.+5., 10.6, \"Forecast\", fontsize=fontsize, color='blue', horizontalalignment='center', verticalalignment='top')\n",
    "\n",
    "plt.plot(year_forecast_obs, forecast_obs, color='lightgrey', lw=lw, label='HadISST')\n",
    "plt.ylabel(forecast_var, fontsize=fontsize)\n",
    "plt.xlim(xlim)\n",
    "# plt.ylim((7.5, 10.5))\n",
    "plt.ylim(ylim)\n",
    "plt.xticks([], fontsize=fontsize)\n",
    "plt.yticks(fontsize=fontsize)\n",
    "plt.axvline(year_forecast_obs[-1], linestyle=':', color='grey', lw=2)\n",
    "plt.text(textx, texty, \"a) Raw forecasts\", fontsize=fontsize, verticalalignment='top')\n",
    "# cb = plt.colorbar(clevs, orientation='vertical')\n",
    "# cb.set_ticks([])\n",
    "# cb.set_label(\"Raw forecasts from analogues (all lead times). Ensemble member density\", fontsize=fontsize*1.2)\n",
    "\n",
    "# LT2-10 in obs and analogue forecast (as that is what we are interested in). Careful with validity times.\n",
    "plt.plot(year_forecast_obs_shifted, forecast_obs_lt210, color='deepskyblue', lw=lw, linestyle='-', label='HadISST\\n(years 2-10)')\n",
    "\n",
    "# HadISST Y2-10 incorporating the 9 months of 2020 as well, to see if it is trending down as we suggest\n",
    "# plt.plot(2020, forecast_obs_lt210_2020, 'x', lw=lw, mew=mew, markersize=markersize)\n",
    "\n",
    "ann_forecast_lt210 = np.ma.mean(ann_forecast_recentred[:, 2:11], axis=1)\n",
    "ind = np.nonzero(forecast_obs_lt210 * ann_forecast_lt210)\n",
    "ann_forecast_lt210_scaled = (ann_forecast_lt210 - np.ma.mean(ann_forecast_lt210[ind])) * (np.ma.std(forecast_obs_lt210[ind]) / np.ma.std(ann_forecast_lt210[ind])) + np.ma.mean(forecast_obs_lt210[ind])\n",
    "\n",
    "ind = np.nonzero(forecast_obs_lt210 * ann_forecast_lt210 * year_forecast_obs_shifted_masked)\n",
    "ann_forecast_lt210_anomadjust = (ann_forecast_lt210 - np.ma.mean(ann_forecast_lt210[ind])) + np.ma.mean(forecast_obs_lt210[ind])\n",
    "\n",
    "# plt.plot(year_forecast_obs_shifted, ann_forecast_lt210_anomadjust, color='red', lw=lw, linestyle='--', label='Analogue forecast\\n(years 2-10, anom. adjusted)')\n",
    "# plt.plot(year_forecast_obs_shifted, ann_forecast_lt210_scaled, color='red', lw=lw, linestyle='--', label='Analogue forecast\\n(years 2-10, normalised)')\n",
    "# plt.legend(loc=0)\n",
    "\n",
    "plt.legend(loc=3, ncol=2, fontsize=fontsize)\n",
    "plt.ylabel(forecast_var, fontsize=fontsize)\n",
    "\n",
    "this_ax = plt.subplot(gs1[1, 0])\n",
    "pos2 = this_ax.get_position()\n",
    "clevs = plt.pcolormesh(year_reshaped_edges, edges, ann_forecast_recentred_kde_contour_norm, cmap=cmap_kde, norm=norm_kde2, rasterized=True)\n",
    "# clevs = plt.pcolormesh(year_reshaped, mids, ann_forecast_recentred_kde_contour_norm, cmap=cmap_kde, norm=norm_kde2, rasterized=True)\n",
    "\n",
    "interval = mids[5] - mids[4]  # This is required to get the 10/50/90% _contours_ to align with the colormesh\n",
    "# plt.contour(year_reshaped, mids+interval/2., cumul_recentred, levels=np.array([0.05, 0.95]), colors='white', linewidths=3)\n",
    "# plt.contour(year_reshaped, mids+interval/2., cumul_recentred, levels=np.array([0.1, 0.9]), colors='white', linewidths=3)\n",
    "# plt.contour(year_reshaped, mids+interval/2., cumul_recentred, levels=np.array([0.5]), colors='white', linewidths=3)\n",
    "\n",
    "plt.plot(year_forecast_obs, forecast_obs, color='lightgrey', lw=lw)#, label='HadISST')\n",
    "plt.ylabel(forecast_var, fontsize=fontsize)\n",
    "plt.xlim(xlim)\n",
    "# plt.ylim((7.5, 10.5))\n",
    "plt.ylim(ylim)\n",
    "plt.xticks(fontsize=fontsize)\n",
    "plt.yticks(fontsize=fontsize)\n",
    "plt.axvline(year_forecast_obs[-1], linestyle=':', color='grey', lw=2)\n",
    "plt.text(textx, texty, \"b) Processed forecasts\", fontsize=fontsize, verticalalignment='top')\n",
    "text = \"ACC skill = {:.2f}\\nSince 1960 ACC skill = {:.2f}\"\n",
    "plt.text(textx2, texty, text.format(ann_forecast_multiskill[-1], ann_forecast_multiskill1960[-1]),\n",
    "         fontsize=fontsize, verticalalignment='top', horizontalalignment='right', color='red')\n",
    "plt.xlabel('Year', fontsize=fontsize)\n",
    "\n",
    "# cb.set_label(\"Processed forecasts from analogues (all lead times). Ensemble member density\", fontsize=fontsize*1.2)\n",
    "\n",
    "# LT2-10 in obs and analogue forecast (as that is what we are interested in). Careful with validity times.\n",
    "plt.plot(year_forecast_obs_shifted, forecast_obs_lt210, color='deepskyblue', lw=lw, linestyle='-')#, label='HadISST\\n(years 2-10)')\n",
    "\n",
    "\n",
    "# DELETE THIS\n",
    "############\n",
    "# savefile = '/home/mmenary/temp.pkl'\n",
    "# with open(savefile, 'rb') as handle:\n",
    "#     historical_time_series_anom_ensmn = pickle.load(handle)\n",
    "# historical_time_series_anom_ensmn_mmm = np.ma.mean(historical_time_series_anom_ensmn, axis=0)\n",
    "# max_lead_multi = 11\n",
    "# forecast = np.ma.masked_all(shape=(nyrs, max_lead_multi))\n",
    "# for ilead in range(max_lead_multi):\n",
    "#     forecast[:nyrs-ilead, ilead] = historical_time_series_anom_ensmn_mmm[ilead:]\n",
    "# plt.plot((year_forecast_obs_shifted)[:-11], np.ma.mean(forecast[:, 2:11], axis=1)[:-11]+forecast_obs_lt210.mean(),\n",
    "#          lw=lw, label='HIST', color='green')\n",
    "\n",
    "savefile = '/home/mmenary/temp2.pkl'\n",
    "nyrs2 = 251\n",
    "year_forecast_obs_shifted2 = np.arange(nyrs2) + 1850 + 10\n",
    "with open(savefile, 'rb') as handle:\n",
    "    historical_time_series_anom_ensmn = pickle.load(handle)\n",
    "historical_time_series_anom_ensmn_mmm = np.ma.mean(historical_time_series_anom_ensmn, axis=0)\n",
    "max_lead_multi = 11\n",
    "forecast = np.ma.masked_all(shape=(nyrs2, max_lead_multi))\n",
    "for ilead in range(max_lead_multi):\n",
    "    forecast[:nyrs2-ilead, ilead] = historical_time_series_anom_ensmn_mmm[ilead:]\n",
    "plt.plot((year_forecast_obs_shifted2)[:-11], np.ma.mean(forecast[:, 2:11], axis=1)[:-11]+forecast_obs_lt210.mean(),\n",
    "         lw=lw, label='Hist. + Scenario\\n(CMIP5+6)', color='green')\n",
    "\n",
    "# hind_ts = hindcast_time_series_ensmn_ltbc_mmm - hindcast_time_series_ensmn_ltbc_mmm[t0:t1, :].mean()\n",
    "# hind_ts = np.ma.mean(hind_ts[:, 2:11], axis=1)\n",
    "# hind_ts += forecast_obs_lt210.mean()\n",
    "# plt.plot(year_forecast_obs_shifted, hind_ts, color='violet', linestyle='-', label='Hindcasts\\n(years 2-10)', lw=2)\n",
    "\n",
    "hind_ts = hindcast_time_series_ensmn_ltbc_mm40m_lt2t10 - hindcast_time_series_ensmn_ltbc_mm40m_lt2t10[t0:t1].mean()\n",
    "hind_ts += forecast_obs_lt210.mean()\n",
    "plt.plot(year_forecast_obs_shifted, hind_ts, color='violet', linestyle='-', label='Hindcasts\\n(years 2-10)', lw=3)\n",
    "############\n",
    "\n",
    "\n",
    "# HadISST Y2-10 incorporating the 9 months of 2020 as well, to see if it is trending down as we suggest\n",
    "plt.plot(2020, forecast_obs_lt210_2020, 'x', color='deepskyblue', lw=lw, mew=mew, markersize=markersize, markerfacecolor='None')\n",
    "# plt.plot(2020, hadisst_spg_2020, 'x', color='lightgrey', lw=lw, mew=mew, markersize=markersize, markerfacecolor='None')\n",
    "\n",
    "# plt.plot(year_forecast_obs_shifted, ann_forecast_lt210_anomadjust, color='red', lw=lw, linestyle='--', label='Analogue forecast\\n(years 2-10, anom. adjusted)')\n",
    "plt.plot(year_forecast_obs_shifted, ann_forecast_lt210_scaled, color='red', lw=lw, linestyle='-', label='Analogue\\n(years 2-10)')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "# Note that if we plot the year-10 we can see whether the \"analogue\" is any different from the underlying obs.\n",
    "# Where it isn't, we know that there is nothing but noise (i.e. the averaging over ensemble members = 0). Where it\n",
    "# is different we know that there is at least some signal (although it isn't necessarilly skilful). This basically\n",
    "# shows that before 1970 we have nothing but noise.\n",
    "# plt.plot(year_forecast_obs_shifted-10, ann_forecast_lt210_scaled, color='red', lw=lw, linestyle='-', label='Analogue\\n(years 2-10)')\n",
    "\n",
    "plt.legend(loc=3, ncol=3, fontsize=fontsize)\n",
    "plt.ylabel(forecast_var, fontsize=fontsize)\n",
    "\n",
    "print pos2\n",
    "\n",
    "cax = plt.gcf().add_axes([0.93, pos2.y0, 0.03, pos1.y1-pos2.y0])\n",
    "cb = plt.colorbar(clevs, cax=cax, orientation='vertical')\n",
    "cb.set_ticks(levels_kde2[::2])\n",
    "cb.ax.tick_params(labelsize=fontsize)\n",
    "cb.set_label(\"Relative ensemble member density\", fontsize=fontsize)\n",
    "\n",
    "print fig_save_file4\n",
    "plt.savefig(fig_save_file4, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Would be good to normalise this against the total number of simulated years for each experiment\n",
    "\n",
    "corr_info = trend_corr_info[:, max_mems_to_take-chosen_num_mem:, :].copy()\n",
    "model_matrix, expt_matrix, models, expts, expts_used = make_info_matrices(corr_info, year_forecast_obs, chosen_num_mem)\n",
    "\n",
    "main_expts = expts[:10]\n",
    "nmain_expts = len(main_expts)\n",
    "print main_expts\n",
    "\n",
    "experiment_count = np.ma.zeros(shape=(nmain_expts+1, nyrs))\n",
    "for jj in range(chosen_num_mem):\n",
    "    for iyr, year in enumerate(year_forecast_obs):\n",
    "        expt_num = expt_matrix[iyr, jj]\n",
    "        if np.ma.is_masked(expt_num):\n",
    "            continue\n",
    "        if expt_num >= nmain_expts:\n",
    "            raise ValueError(expt_num)\n",
    "        experiment_count[expt_num+1, iyr] += 1\n",
    "\n",
    "experiment_count_cumsum = np.cumsum(experiment_count, axis=0)\n",
    "ind = np.argwhere(experiment_count_cumsum[-1, :] < chosen_num_mem)\n",
    "if len(ind) > 0:\n",
    "    experiment_count_cumsum[:, :ind[-1][0]+1] = np.ma.masked\n",
    "\n",
    "forecast_obs_scaled = (forecast_obs - forecast_obs.mean()) * ((chosen_num_mem / 10.) / forecast_obs.std()) + chosen_num_mem / 2.\n",
    "\n",
    "expt_colors2 = ['grey', 'orange', 'green', 'cyan', 'cyan', 'cyan', 'red', 'red', 'red', 'red']\n",
    "# expt_colors2 = ['grey', 'orange', 'cyan', 'cyan', 'cyan', 'cyan', 'red', 'red', 'violet', 'violet']\n",
    "\n",
    "alpha = 1\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for iexpt, expt in enumerate(main_expts):\n",
    "    lower = experiment_count_cumsum[iexpt]\n",
    "    upper = experiment_count_cumsum[iexpt+1]\n",
    "    plt.fill_between(year_forecast_obs, lower, upper, alpha=alpha, color=expt_colors2[iexpt], label=expt)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(0, chosen_num_mem)\n",
    "plt.xlabel('Year', fontsize=fontsize)\n",
    "plt.xticks(fontsize=fontsize)\n",
    "plt.yticks(np.arange(chosen_num_mem+1), fontsize=fontsize)\n",
    "plt.legend(loc=3)\n",
    "\n",
    "plt.plot(year_forecast_obs, forecast_obs_scaled, color='k', label='Obs (TRUTH)')\n",
    "plt.axhline(10, linestyle=':', color='k')\n",
    "\n",
    "print fig_save_file3\n",
    "plt.savefig(fig_save_file3, bbox_inches = 'tight')\n",
    "\n",
    "corr_info = ann_corr_info[:, max_mems_to_take-chosen_num_mem:, :].copy()\n",
    "model_matrix, expt_matrix, models, expts, expts_used = make_info_matrices(corr_info, year_forecast_obs, chosen_num_mem)\n",
    "\n",
    "experiment_count = np.ma.zeros(shape=(nmain_expts+1, nyrs))\n",
    "for jj in range(chosen_num_mem):\n",
    "    for iyr, year in enumerate(year_forecast_obs):\n",
    "        expt_num = expt_matrix[iyr, jj]\n",
    "        if np.ma.is_masked(expt_num):\n",
    "            continue\n",
    "        if expt_num >= nmain_expts:\n",
    "            raise ValueError(expt_num)\n",
    "        experiment_count[expt_num+1, iyr] += 1\n",
    "\n",
    "experiment_count_cumsum = np.cumsum(experiment_count, axis=0)\n",
    "ind = np.argwhere(experiment_count_cumsum[-1, :] < chosen_num_mem)\n",
    "if len(ind) > 0:\n",
    "    experiment_count_cumsum[:, :ind[-1][0]+1] = np.ma.masked\n",
    "\n",
    "forecast_obs_scaled = (forecast_obs - forecast_obs.mean()) * ((chosen_num_mem / 10.) / forecast_obs.std()) + chosen_num_mem / 2.\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for iexpt, expt in enumerate(main_expts):\n",
    "    lower = experiment_count_cumsum[iexpt]\n",
    "    upper = experiment_count_cumsum[iexpt+1]\n",
    "    plt.fill_between(year_forecast_obs, lower, upper, alpha=alpha, color=expt_colors2[iexpt], label=expt)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(0, chosen_num_mem)\n",
    "plt.xlabel('Year', fontsize=fontsize)\n",
    "plt.xticks(fontsize=fontsize)\n",
    "plt.yticks(np.arange(chosen_num_mem+1), fontsize=fontsize)\n",
    "plt.legend(loc=3)\n",
    "\n",
    "plt.plot(year_forecast_obs, forecast_obs_scaled, color='k', label='Obs (TRUTH)')\n",
    "plt.axhline(10, linestyle=':', color='k')\n",
    "\n",
    "print fig_save_file3b\n",
    "plt.savefig(fig_save_file3b, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============\n",
    "# As above but nicer looking - just for the ANN version\n",
    "# ===============\n",
    "\n",
    "year_model_historical = 1850 + np.arange(251)\n",
    "method = 'ann'\n",
    "\n",
    "corr_info = ann_corr_info[:, max_mems_to_take-chosen_num_mem:, :].copy()\n",
    "model_matrix, expt_matrix, models, expts, expts_used = make_info_matrices(corr_info, year_forecast_obs, chosen_num_mem)\n",
    "#                         print models\n",
    "models_padded = models[:]\n",
    "for jj, model in enumerate(models):\n",
    "#                             print jj, model\n",
    "    if (jj % 2) == 0:\n",
    "        models_padded[jj] = model + ' ' * 25\n",
    "\n",
    "# within this range for historical then make different colour\n",
    "tolerance1 = 20  \n",
    "tolerance2 = 10\n",
    "tolerance3 = 3\n",
    "hist_colors = ['pink', 'purple', 'red']\n",
    "# hist_colors = ['orange', 'orange', 'orange']\n",
    "\n",
    "expts_sorted = expts[:]\n",
    "expt_colors_sorted = expt_colors[:]\n",
    "\n",
    "for color, tolerance in zip(hist_colors, [tolerance3, tolerance2, tolerance1]):\n",
    "    text = 'historical\\n(within {:d} years)'.format(tolerance)\n",
    "    expts_sorted.insert(2, text)\n",
    "    expt_colors_sorted.insert(2, color)\n",
    "    expts_used.insert(2, text)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "for jj in range(chosen_num_mem):\n",
    "#     if jj > 5: continue\n",
    "    for iyr, year in enumerate(year_forecast_obs):\n",
    "        model_num = model_matrix[iyr, jj]\n",
    "        if (model_num % 3) == 0:\n",
    "            marker = 'x'\n",
    "        elif (model_num % 3) == 1:\n",
    "            marker = 'o'\n",
    "        elif (model_num % 3) == 2:\n",
    "            marker = '^'\n",
    "        else:\n",
    "            continue  # If masked\n",
    "            \n",
    "        color = expt_colors[expt_matrix[iyr, jj]]\n",
    "        if (expts_used[expt_matrix[iyr, jj]] == 'historical'):\n",
    "            this_index = corr_info[iyr, jj, 4]\n",
    "            year_diff = np.abs(year_model_historical[this_index] - year)\n",
    "            if year_diff < tolerance3:\n",
    "                color = hist_colors[0]\n",
    "            elif year_diff < tolerance2:\n",
    "                color = hist_colors[1]\n",
    "            elif year_diff < tolerance1:\n",
    "                color = hist_colors[2]\n",
    "            \n",
    "        plt.plot(year_forecast_obs[iyr], model_num, marker, color=color, markerfacecolor='None', mew=2, markersize=7)\n",
    "        \n",
    "plt.xlabel('Year', fontsize=fontsize)\n",
    "plt.xticks(fontsize=fontsize)\n",
    "plt.yticks(np.arange(len(models)))\n",
    "plt.gca().set_yticklabels(models_padded, fontsize=10)\n",
    "plt.title('{:s} method: The model(s) and experiment(s) used in each window'.format(method), fontsize=fontsize)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(-1, len(models))\n",
    "for num in range(0, len(models), 3):\n",
    "    plt.axhline(num, linestyle=':', color='grey')\n",
    "\n",
    "custom_lines = []\n",
    "for jj, this_expt in enumerate(expts_sorted):\n",
    "    if this_expt not in expts_used:\n",
    "        continue\n",
    "    custom_lines.append(Line2D([0], [0], color=expt_colors_sorted[jj], lw=2))\n",
    "plt.legend(custom_lines, expts_used, loc=3, fontsize=fontsize, handlelength=1.2)\n",
    "\n",
    "print fig_save_file1d\n",
    "plt.savefig(fig_save_file1d, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure how useful/relevant this is - this is the END of the window/start of the forecast (i.e. lead=0)\n",
    "\n",
    "# This could do with tidying up if we want to keep it\n",
    "corr_info = ann_corr_info[:, max_mems_to_take-chosen_num_mem:, :].copy()\n",
    "print corr_info.shape\n",
    "print corr_info[100, 0, :]\n",
    "\n",
    "corr_info_hist_years = np.ma.masked_all(shape=(chosen_num_mem, nyrs))  # The hist-year that was chosen\n",
    "corr_info_hist_years_diff = np.ma.masked_all(shape=(chosen_num_mem, nyrs))  # The diff to the REAL hist-year\n",
    "for iyr, year in enumerate(year_forecast_obs):\n",
    "    for imem in range(chosen_num_mem):\n",
    "        this_expt = corr_info[iyr, imem, 2]\n",
    "        if this_expt != \"historical\":\n",
    "            continue\n",
    "        this_index = corr_info[iyr, imem, 4]\n",
    "        if not np.ma.is_masked(this_index):\n",
    "#             print this_index\n",
    "            corr_info_hist_years[imem, iyr] = year_model_historical[this_index]\n",
    "            corr_info_hist_years_diff[imem, iyr] = year_model_historical[this_index] - year\n",
    "\n",
    "corr_info_hist_years_diff_mn = np.ma.mean(corr_info_hist_years_diff, axis=0)\n",
    "mn = np.ma.mean(corr_info_hist_years_diff_mn)\n",
    "sd = np.ma.std(corr_info_hist_years_diff_mn)\n",
    "sd_scaling = chosen_num_mem / 7.\n",
    "centreline = chosen_num_mem / 2.\n",
    "corr_info_hist_years_diff_mn_scaled = (corr_info_hist_years_diff_mn - mn) * (sd_scaling / sd) + centreline\n",
    "\n",
    "# Bootstrap some shuffled data to see what the expected downwards trend is\n",
    "nboot = 200\n",
    "corr_info_hist_years_diff_mn_scaled_random = np.ma.masked_all(shape=(nboot, nyrs))\n",
    "for iboot in range(nboot):\n",
    "    corr_info_hist_years_random = np.zeros(shape=corr_info_hist_years.shape)\n",
    "    for imem in range(chosen_num_mem):\n",
    "        corr_info_hist_years_random[imem, chosen_window:] = np.random.permutation(corr_info_hist_years[imem, chosen_window:])\n",
    "    corr_info_hist_years_random = np.ma.masked_less(corr_info_hist_years_random, 1.)\n",
    "    corr_info_hist_years_random = np.ma.masked_greater(corr_info_hist_years_random, 2050)\n",
    "    corr_info_hist_years_diff_random = np.ma.masked_all(shape=corr_info_hist_years.shape)\n",
    "    for iyr, year in enumerate(year_forecast_obs):\n",
    "        corr_info_hist_years_diff_random[:, iyr] = corr_info_hist_years_random[:, iyr] - year\n",
    "\n",
    "    corr_info_hist_years_diff_mn_random = np.ma.mean(corr_info_hist_years_diff_random, axis=0)\n",
    "    mn = np.ma.mean(corr_info_hist_years_diff_mn_random)\n",
    "    sd = np.ma.std(corr_info_hist_years_diff_mn_random)\n",
    "    sd_scaling = chosen_num_mem / 7.\n",
    "    centreline = chosen_num_mem / 2.\n",
    "    corr_info_hist_years_diff_mn_scaled_random[iboot, :] = (corr_info_hist_years_diff_mn_random - mn) * (sd_scaling / sd) + centreline\n",
    "corr_info_hist_years_diff_mn_scaled_random_mn = np.ma.mean(corr_info_hist_years_diff_mn_scaled_random, axis=0)\n",
    "\n",
    "plt.figure(figsize=(15, 20))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.pcolormesh(year_forecast_obs, np.arange(chosen_num_mem), corr_info_hist_years)\n",
    "plt.colorbar()\n",
    "\n",
    "cmap = plt.get_cmap('coolwarm')\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.pcolormesh(year_forecast_obs, np.arange(chosen_num_mem), corr_info_hist_years_diff, cmap=cmap, vmin=-50, vmax=50)\n",
    "plt.plot(year_forecast_obs, corr_info_hist_years_diff_mn_scaled, color='k', lw=3)\n",
    "plt.plot(year_forecast_obs, corr_info_hist_years_diff_mn_scaled_random_mn, color='k', lw=4, linestyle=':')\n",
    "plt.axhline(centreline, color='k', linestyle='--', lw=2)\n",
    "plt.colorbar()\n",
    "title = 'Analogue hist year - real hist year. Red means analogue is LATER on than reality\\n' + \\\n",
    "        'Dotted: Would expect a gradual trend from red to blue in the case of just noise'\n",
    "plt.title(title)\n",
    "\n",
    "cmap = plt.get_cmap('viridis')\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.pcolormesh(year_forecast_obs, np.arange(chosen_num_mem), np.abs(corr_info_hist_years_diff), cmap=cmap, vmin=0, vmax=50)\n",
    "plt.colorbar()\n",
    "plt.title('As above but for absolute values')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Would be good to normalise this against the total number of simulated years for each experiment\n",
    "\n",
    "main_expts = expts[:10]\n",
    "nmain_expts = len(main_expts)\n",
    "print main_expts\n",
    "\n",
    "corr_info = trend_corr_info[:, max_mems_to_take-chosen_num_mem:, :]\n",
    "model_matrix, expt_matrix, models, expts, expts_used = make_info_matrices(corr_info, year_forecast_obs, chosen_num_mem)\n",
    "\n",
    "experiment_count = np.ma.zeros(shape=(nmain_expts+1, nyrs))\n",
    "for jj in range(chosen_num_mem):\n",
    "    for iyr, year in enumerate(year_forecast_obs):\n",
    "        expt_num = expt_matrix[iyr, jj]\n",
    "        if np.ma.is_masked(expt_num):\n",
    "            continue\n",
    "        if expt_num >= nmain_expts:\n",
    "            raise ValueError(expt_num)\n",
    "        experiment_count[expt_num+1, iyr] += 1\n",
    "\n",
    "experiment_count_cumsum = np.cumsum(experiment_count, axis=0)\n",
    "ind = np.argwhere(experiment_count_cumsum[-1, :] < chosen_num_mem)[-1][0]\n",
    "\n",
    "print len(np.argwhere(experiment_count_cumsum[-1, :] < chosen_num_mem))\n",
    "print ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print corr_info_hist_years_diff_random[:, 148]\n",
    "print np.ma.mean(corr_info_hist_years_diff_random[:, 148])\n",
    "print corr_info_hist_years_random[:, 148]\n",
    "print corr_info_hist_years_diff_mn_random[148]\n",
    "\n",
    "print corr_info_hist_years.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import analogue\n",
    "# reload(analogue)\n",
    "# from analogue import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================\n",
    "# Plot the lead time dependent skill wrt baseline measures\n",
    "# Bottom right plot is the relevant one...\n",
    "# =================\n",
    "fontsize = 16\n",
    "#                     ylim = (-0.3, 1.1)\n",
    "ylim = (-0.1, 1.1)\n",
    "lw = 3\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_baseline_data(baseline_save_file, multi=False)\n",
    "for num in [-1, 0, 1]:\n",
    "    plt.axhline(num, linestyle=':', color='k')\n",
    "plt.plot(lead_times, trend_forecast_skill, color='red', label='Analogue (trend-based)', lw=lw)\n",
    "plt.plot(lead_times, trend_forecast_skill1960, color='red', linestyle='--', label='Analogue (trend-based) (since 1960)', lw=lw)\n",
    "plt.plot(lead_times, trend_forecast_res_skill, color='red', linestyle=':', label='Analogue (trend-based) RESIDUAL', lw=lw)\n",
    "plt.plot(lead_times, trend_forecast_skill1990, color='red', linestyle='-.', label='Analogue (trend-based) (BEFORE 1990)', lw=lw)\n",
    "plt.plot(lead_times, random_forecast_skill, color='orange', linestyle='-', label='Random noise', lw=lw)\n",
    "plt.ylabel('Correlation', fontsize=fontsize)\n",
    "plt.xlabel('Lead time [years]', fontsize=fontsize)\n",
    "plt.xticks(np.arange(nlead), fontsize=fontsize)\n",
    "plt.yticks(fontsize=fontsize)\n",
    "plt.title('TREND: Target: {:s}\\nLead time dependent skill'.format(chosen_target_region), fontsize=fontsize)\n",
    "plt.ylim(ylim)\n",
    "plt.xlim(0, 10)\n",
    "plt.legend(loc=2, ncol=3, fontsize=fontsize*0.8, bbox_to_anchor=(0, -0.1))\n",
    "\n",
    "index_sets = [[0, 1, 2, 3, 4, 5], [6, 7]]\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_baseline_data(baseline_save_file, multi=True, lead_times_multi=lead_times_multi)\n",
    "for ii, ind in enumerate(index_sets):\n",
    "    for num in [-1, 0, 1]:\n",
    "        plt.axhline(num, linestyle=':', color='k')\n",
    "    plt.plot(lead_times_multi[ind], trend_forecast_multiskill[ind], color='red', lw=lw)\n",
    "    plt.plot(lead_times_multi[ind], trend_forecast_multiskill1960[ind], color='red', linestyle='--', lw=lw)\n",
    "    plt.plot(lead_times_multi[ind], trend_forecast_multiskill1990[ind], color='red', linestyle='-.', lw=lw)\n",
    "    plt.plot(lead_times_multi[ind], trend_forecast_res_multiskill[ind], color='red', linestyle=':', lw=lw)\n",
    "    plt.plot(lead_times_multi[ind], random_forecast_multiskill[ind], color='orange', linestyle='-', lw=lw)\n",
    "plt.xlabel('Lead time [MULTIPLE years]', fontsize=fontsize)\n",
    "plt.xticks(np.arange(len(start_lead)), fontsize=fontsize)\n",
    "plt.gca().set_xticklabels(labels=labels_multi)\n",
    "plt.yticks([], fontsize=fontsize)\n",
    "plt.title('TREND: Target: {:s}\\nMulti-annual lead time dependent skill'.format(chosen_target_region), fontsize=fontsize)\n",
    "plt.ylim(ylim)\n",
    "\n",
    "print fig_save_file2\n",
    "plt.savefig(fig_save_file2, bbox_inches = 'tight')\n",
    "# plt.close(plt.gcf())\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_baseline_data(baseline_save_file, multi=False)\n",
    "for num in [-1, 0, 1]:\n",
    "    plt.axhline(num, linestyle=':', color='k')\n",
    "plt.plot(lead_times, ann_forecast_skill, color='red', label='Analogue (ann-based)', lw=lw)\n",
    "plt.plot(lead_times, ann_forecast_skill1960, color='red', linestyle='--', label='Analogue (ann-based) (since 1960)', lw=lw)\n",
    "plt.plot(lead_times, ann_forecast_res_skill, color='red', linestyle=':', label='Analogue (ann-based) RESIDUAL', lw=lw)\n",
    "plt.plot(lead_times, ann_forecast_skill1990, color='red', linestyle='-.', label='Analogue (ann-based) (BEFORE 1990)', lw=lw)\n",
    "plt.plot(lead_times, random_forecast_skill, color='orange', linestyle='-', label='Random noise', lw=lw)\n",
    "plt.plot(lead_times, random_forecast_skill1960, color='orange', linestyle='--', label='Random noise (since 1960)', lw=lw)\n",
    "plt.plot(lead_times, random_forecast_skill1990, color='orange', linestyle='-.', label='Random noise (before 1990)', lw=lw)\n",
    "plt.ylabel('Correlation', fontsize=fontsize)\n",
    "plt.xlabel('Lead time [years]', fontsize=fontsize)\n",
    "plt.xticks(np.arange(nlead), fontsize=fontsize)\n",
    "plt.yticks(fontsize=fontsize)\n",
    "plt.title('ANN: Target: {:s}\\nLead time dependent skill'.format(chosen_target_region), fontsize=fontsize)\n",
    "plt.ylim(ylim)\n",
    "plt.xlim(0, 10)\n",
    "plt.legend(loc=2, ncol=3, fontsize=fontsize*0.8, bbox_to_anchor=(0, -0.1))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_baseline_data(baseline_save_file, multi=True, lead_times_multi=lead_times_multi)\n",
    "for ii, ind in enumerate(index_sets):\n",
    "    for num in [-1, 0, 1]:\n",
    "        plt.axhline(num, linestyle=':', color='k')\n",
    "    plt.plot(lead_times_multi[ind], ann_forecast_multiskill[ind], color='red', lw=lw)\n",
    "    plt.plot(lead_times_multi[ind], ann_forecast_multiskill1960[ind], color='red', linestyle='--', lw=lw)\n",
    "    plt.plot(lead_times_multi[ind], ann_forecast_multiskill1990[ind], color='red', linestyle='-.', lw=lw)\n",
    "    plt.plot(lead_times_multi[ind], ann_forecast_res_multiskill[ind], color='red', linestyle=':', lw=lw)\n",
    "    plt.plot(lead_times_multi[ind], random_forecast_multiskill[ind], color='orange', linestyle='-', lw=lw)\n",
    "    plt.plot(lead_times_multi[ind], random_forecast_multiskill1960[ind], color='orange', linestyle='--', lw=lw)\n",
    "    plt.plot(lead_times_multi[ind], random_forecast_multiskill1990[ind], color='orange', linestyle='-.', lw=lw)\n",
    "plt.xlabel('Lead time [MULTIPLE years]', fontsize=fontsize)\n",
    "plt.xticks(np.arange(len(start_lead)), fontsize=fontsize)\n",
    "plt.gca().set_xticklabels(labels=labels_multi)\n",
    "plt.yticks([], fontsize=fontsize)\n",
    "plt.title('ANN: Target: {:s}\\nMulti-annual lead time dependent skill'.format(chosen_target_region), fontsize=fontsize)\n",
    "plt.ylim(ylim)\n",
    "\n",
    "print fig_save_file2b\n",
    "plt.savefig(fig_save_file2b, bbox_inches = 'tight')\n",
    "# plt.close(plt.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ann_forecast_multiskill1960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(baseline_save_file, 'rb') as handle:\n",
    "    print('Reading from {:s}'.format(baseline_save_file))\n",
    "    baseline_data = pickle.load(handle)\n",
    "    hist_models = baseline_data['hist_models']\n",
    "    hind_models = baseline_data['hind_models']\n",
    "    historical_mmm = baseline_data['historical_mmm']\n",
    "    historical_anom_ensmn_corr = baseline_data['historical_anom_ensmn_corr']\n",
    "    historical_anom_ensmn_mmm_corr = baseline_data['historical_anom_ensmn_mmm_corr']\n",
    "    historical_anom_ensmn_mmm1960_corr = baseline_data['historical_anom_ensmn_mmm1960_corr']\n",
    "    historical_anom_ensmn_mmm1990_corr = baseline_data['historical_anom_ensmn_mmm1990_corr']\n",
    "    historical_anom_ensmn_mmmsub_multicorr = baseline_data['historical_anom_ensmn_mmmsub_multicorr']\n",
    "    historical_anom_ensmn_mmmsub1960_multicorr = baseline_data['historical_anom_ensmn_mmmsub1960_multicorr']\n",
    "    hindcast_time_series_ensmn_mmm_corr = baseline_data['hindcast_time_series_ensmn_mmm_corr']\n",
    "    hindcast_time_series_ensmn_ltbc_mmm_corr = baseline_data['hindcast_time_series_ensmn_ltbc_mmm_corr']\n",
    "    historical_anom_inf_ensmn_mmm_corr = baseline_data['historical_anom_inf_ensmn_mmm_corr']\n",
    "    persistence_corr = baseline_data['persistence_corr']\n",
    "    persistence_res_corr = baseline_data['persistence_res_corr']\n",
    "    persistence_smoothed_corr = baseline_data['persistence_smoothed_corr']\n",
    "    persistence1960_corr = baseline_data['persistence1960_corr']\n",
    "    persistence1990_corr = baseline_data['persistence1990_corr']\n",
    "    historical_anom_ensmn_multicorr = baseline_data['historical_anom_ensmn_multicorr']\n",
    "    historical_anom_ensmn_mmm_multicorr = baseline_data['historical_anom_ensmn_mmm_multicorr']\n",
    "    historical_anom_ensmn_mmm1960_multicorr = baseline_data['historical_anom_ensmn_mmm1960_multicorr']\n",
    "    # historical_anom_ensmn_mmm1990_multicorr = baseline_data['historical_anom_ensmn_mmm1990_multicorr']\n",
    "    hindcast_time_series_ensmn_mmm_multicorr = baseline_data['hindcast_time_series_ensmn_mmm_multicorr']\n",
    "    hindcast_time_series_ensmn_ltbc_mmm_multicorr = baseline_data['hindcast_time_series_ensmn_ltbc_mmm_multicorr']\n",
    "    hindcast_time_series_ensmn_ltbc_mmm5_multicorr = baseline_data['hindcast_time_series_ensmn_ltbc_mmm5_multicorr']\n",
    "    hindcast_time_series_ensmn_ltbc_mmm6_multicorr = baseline_data['hindcast_time_series_ensmn_ltbc_mmm6_multicorr']\n",
    "    historical_anom_inf_ensmn_mmm_multicorr = baseline_data['historical_anom_inf_ensmn_mmm_multicorr']\n",
    "    persistence_multicorr = baseline_data['persistence_multicorr']\n",
    "    persistence_res_multicorr = baseline_data['persistence_res_multicorr']\n",
    "    persistence_smoothed_multicorr = baseline_data['persistence_smoothed_multicorr']\n",
    "    persistence1960_multicorr = baseline_data['persistence1960_multicorr']\n",
    "    hindcast_time_series_ensmn_mmm_res_ltbc_corr = baseline_data['hindcast_time_series_ensmn_mmm_res_ltbc_corr']\n",
    "    hindcast_time_series_ensmn_mmm_res_ltbc_multicorr = baseline_data['hindcast_time_series_ensmn_mmm_res_ltbc_multicorr']\n",
    "    mpi_ensmn_corr = baseline_data['mpi_ensmn_corr']\n",
    "    mpi_ensmn1960_corr = baseline_data['mpi_ensmn1960_corr']\n",
    "    mpi_ensmn1990_corr = baseline_data['mpi_ensmn1990_corr']\n",
    "    mpi_ensmn_multicorr = baseline_data['mpi_ensmn_multicorr']\n",
    "    mpi_ensmn1960_multicorr = baseline_data['mpi_ensmn1960_multicorr']\n",
    "    mpi_ensmn1990_multicorr = baseline_data['mpi_ensmn1990_multicorr']\n",
    "        \n",
    "# import analogue\n",
    "# reload(analogue)\n",
    "# from analogue import *\n",
    "\n",
    "fontsize = 16\n",
    "lw1 = 3\n",
    "lw2 = 2\n",
    "\n",
    "plt.figure(figsize=(6, 7))\n",
    "for ii, ind in enumerate(index_sets):\n",
    "    if ii == 0:\n",
    "        label_anal = 'Analogue'\n",
    "        label_anal1960 = 'Analogue (since 1960)'\n",
    "        label_rand = 'Shuffled'\n",
    "        label_rand1960 = 'Shuffled (since 1960)'\n",
    "        historical_anom_ensmn_mmm_multicorr_label = 'CMIP5+6 hist. (MMM)'\n",
    "        historicalsub_anom_ensmn_mmm_multicorr_label = 'CMIP5+6 hist. Sub (MMM)'\n",
    "        hindcast_time_series_ensmn_ltbc_mmm_multicorr_label = 'CMIP5+6 init.\\n(since 1960, MMM)'\n",
    "        hindcast_time_series_ensmn_ltbc_mmm5_multicorr_label = 'CMIP5 init.\\n(since 1960, MMM)'\n",
    "        hindcast_time_series_ensmn_ltbc_mmm6_multicorr_label = 'CMIP6 init.\\n(since 1960, MMM)'\n",
    "        historical_anom_ensmn_mmm1960_multicorr_label = 'CMIP5+6 hist.\\n(since 1960, MMM)'\n",
    "        persistence_multicorr_label = 'Persistence (obs)'\n",
    "        persistence1960_multicorr_label = 'Persistence\\n(since 1960, obs)'\n",
    "        mpi_ensmn_multicorr_label = 'MPI long init.\\n(3 mems)'\n",
    "        mpi_ensmn1960_multicorr_label = 'MPI long init.\\n(since 1960, mems)'\n",
    "    else:\n",
    "        label_anal = ''\n",
    "        label_anal1960 = ''\n",
    "        label_rand = ''\n",
    "        label_rand1960 = ''\n",
    "        historical_anom_ensmn_mmm_multicorr_label = ''\n",
    "        historicalsub_anom_ensmn_mmm_multicorr_label = ''\n",
    "        hindcast_time_series_ensmn_ltbc_mmm_multicorr_label = ''\n",
    "        hindcast_time_series_ensmn_ltbc_mmm5_multicorr_label = ''\n",
    "        hindcast_time_series_ensmn_ltbc_mmm6_multicorr_label = ''\n",
    "        historical_anom_ensmn_mmm1960_multicorr_label = ''\n",
    "        persistence_multicorr_label = ''\n",
    "        persistence1960_multicorr_label = ''\n",
    "        mpi_ensmn_multicorr_label = ''\n",
    "        mpi_ensmn1960_multicorr_label = ''\n",
    "    plt.plot(lead_times_multi[ind], ann_forecast_multiskill[ind], color='red', lw=lw1, label=label_anal)\n",
    "    plt.plot(lead_times_multi[ind], random_forecast_multiskill[ind], color='orange', linestyle='-', lw=lw2, label=label_rand)\n",
    "    plt.plot(lead_times_multi[ind], historical_anom_ensmn_mmm_multicorr[ind], color='green', lw=lw2, label=historical_anom_ensmn_mmm_multicorr_label)\n",
    "    plt.plot(lead_times_multi[ind], historical_anom_ensmn_mmmsub_multicorr[ind], color='green', lw=lw2*1.5, label=historicalsub_anom_ensmn_mmm_multicorr_label)\n",
    "    plt.plot(lead_times_multi[ind], persistence_multicorr[ind], color='dimgrey', lw=lw2, label=persistence_multicorr_label)\n",
    "    plt.plot(lead_times_multi[ind], mpi_ensmn_multicorr[ind], color='blue', lw=lw2, label=mpi_ensmn_multicorr_label)\n",
    "    plt.plot([0], color='white', label=' ')\n",
    "\n",
    "    plt.plot(lead_times_multi[ind], ann_forecast_multiskill1960[ind], color='red', linestyle='--', lw=lw1, label=label_anal1960)\n",
    "    plt.plot(lead_times_multi[ind], random_forecast_multiskill1960[ind], color='orange', linestyle='--', lw=lw2, label=label_rand1960)\n",
    "    plt.plot(lead_times_multi[ind], historical_anom_ensmn_mmm1960_multicorr[ind], color='green', linestyle='--', lw=lw2, label=historical_anom_ensmn_mmm1960_multicorr_label)\n",
    "    plt.plot(lead_times_multi[ind], historical_anom_ensmn_mmmsub1960_multicorr[ind], color='green', linestyle='--', lw=lw2*1.5, label=historicalsub_anom_ensmn_mmm_multicorr_label)\n",
    "    plt.plot(lead_times_multi[ind], persistence1960_multicorr[ind], color='dimgrey', linestyle='--', lw=lw2, label=persistence1960_multicorr_label)\n",
    "    plt.plot(lead_times_multi[ind], mpi_ensmn1960_multicorr[ind], color='blue', linestyle='--', lw=lw2, label=mpi_ensmn1960_multicorr_label)\n",
    "\n",
    "    plt.plot(lead_times_multi[ind], hindcast_time_series_ensmn_ltbc_mmm_multicorr[ind], linestyle='--', lw=lw1, color='indigo', label=hindcast_time_series_ensmn_ltbc_mmm_multicorr_label)\n",
    "    plt.plot(lead_times_multi[ind], hindcast_time_series_ensmn_ltbc_mmm5_multicorr[ind], linestyle=':', lw=lw1, color='indigo', label=hindcast_time_series_ensmn_ltbc_mmm5_multicorr_label)\n",
    "    plt.plot(lead_times_multi[ind], hindcast_time_series_ensmn_ltbc_mmm6_multicorr[ind], linestyle='-.', lw=lw1, color='indigo', label=hindcast_time_series_ensmn_ltbc_mmm6_multicorr_label)\n",
    "\n",
    "\n",
    "plt.xlabel('Lead time [multiple years]', fontsize=fontsize)\n",
    "plt.xticks(np.arange(len(start_lead)), fontsize=fontsize)\n",
    "plt.gca().set_xticklabels(labels=labels_multi)\n",
    "# plt.yticks([], fontsize=fontsize)\n",
    "plt.ylabel('Multi-annual lead time dependent skill', fontsize=fontsize)\n",
    "plt.ylim([0.35, 0.95])\n",
    "plt.yticks(fontsize=fontsize)\n",
    "\n",
    "plt.legend(loc=2, ncol=1, fontsize=fontsize, bbox_to_anchor=(1, 1.03))\n",
    "\n",
    "print fig_save_file2b\n",
    "plt.savefig(fig_save_file2b, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Jaspy",
   "language": "python",
   "name": "jaspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
